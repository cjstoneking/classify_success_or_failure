{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detecting descriptions of failure in text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this project is to build a system that can detect parts of text that describe instances of failure - such as failure of a project, a piece of equipment, or a company. This problem resembles sentiment analysis, and I will be using some approaches from sentiment analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "googlemaps_api_key = \"not a valid key\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sqlite3\n",
    "import requests\n",
    "import urllib.request\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Assemble training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step is to assemble a set of labeled text data for training the algorithm. I plan to use an algorithm that takes single sentences or parts of sentences as input, and returns an estimated probability that the sentence describes an instance of failure. Therefore, I need a training dataset consisting of sentences with binary labels. I refer to these sentences as positive cases if they describe failure, and negative cases if they do not.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To acquire the positive cases (sentences describing failure), I manually extracted sentences from a variety of texts. These included descriptions of failed construction projects, failed software engineering projects, failed charitable initiatives, failed startups, and other instances of failure. The websites included Medium, Quora, calleam.com and several others. Using multiple sources is crucial, because it helps to prevent the algorithm from learning any spurious associations between the language style of a sentence and its failure-related status. A full list of sources is given in the file of positive cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load positive cases (sentences describing an instance of failure)\n",
    "#these sentences are accumulated as keys in a python dict that is written to a file as raw code\n",
    "#(so it is easy to modify directly)\n",
    "\n",
    "from training_positive_cases import training_positive_cases\n",
    "training_positive_cases = list(training_positive_cases.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obamacare Website Programmers Complained About Unrealistic Deadlines\n",
      "\n",
      "Ignoring users is a tried and true way to fail\n",
      "\n",
      "the President had to admit that the performance of the system was below what would be expected\n",
      "\n",
      "Despite significant technical problems with the prototype\n",
      "\n",
      "they never spend any money promoting it and it goes unused and is left to die\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#print a few cases at random\n",
    "_=[print(s + \"\\n\") for s in np.array(training_positive_cases)[np.random.permutation(len(training_positive_cases))[0:5]]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The negative cases are sentences/parts of sentences that do not describe failure, so these belong to a much larger and more diverse set. They need to resemble the positive cases in terms of style, general language use, and non-failure-related vocabulary, because if there is any systematic difference the algorithm could learn a spurious associations. To obtain the negative cases, I downloaded the text of multiple Wikipedia articles on specific software and other projects which are not known for failure, and used all sentences from the main body of text of these articles as negative cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "downloading https://en.wikipedia.org/wiki/Linux\n",
      "\n",
      "\n",
      "downloading https://en.wikipedia.org/wiki/Triborough_Bridge\n",
      "\n",
      "\n",
      "downloading https://en.wikipedia.org/wiki/Database\n",
      "\n",
      "\n",
      "downloading https://en.wikipedia.org/wiki/Lean_startup\n",
      "\n",
      "\n",
      "downloading https://en.wikipedia.org/wiki/Business_model\n",
      "\n",
      "\n",
      "downloading https://en.wikipedia.org/wiki/Pinterest\n",
      "\n",
      "\n",
      "downloading https://en.wikipedia.org/wiki/Twitter\n",
      "\n",
      "\n",
      "downloading https://en.wikipedia.org/wiki/Application_software\n",
      "\n",
      "\n",
      "downloading https://en.wikipedia.org/wiki/Web_search_engine\n",
      "\n",
      "\n",
      "downloading https://en.wikipedia.org/wiki/Software\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from training_negative_urls import training_negative_urls\n",
    "\n",
    "#download the full text from the specified URLs\n",
    "#and save it to sqlite database\n",
    "#reasons: \n",
    "#         1) avoid downloading multiple times \n",
    "#         2) now have a working snapshot, will not be affected by future wikipedia edits\n",
    "conn = sqlite3.connect('negative_raw_html.sqlite')\n",
    "cur = conn.cursor()\n",
    "cur.execute('CREATE TABLE IF NOT EXISTS Negative (url TEXT, text TEXT) ')\n",
    "\n",
    "def insert_negative_text(cur, url, text):\n",
    "    cur.execute('SELECT text FROM Negative WHERE url = ?', (url,))\n",
    "    #? -> avoid SQL injection\n",
    "    row = cur.fetchone()\n",
    "    if(row is None):\n",
    "        cur.execute('''INSERT INTO Negative (url, text)\n",
    "            VALUES (?, ?)''', (url, text))\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "for url in training_negative_urls.keys():\n",
    "    print('\\ndownloading '+url+'\\n')\n",
    "    article = requests.get(url)\n",
    "    time.sleep(1)  \n",
    "    insert_negative_text(cur, url, article.text)\n",
    "    conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_wikipedia_text(article_text):\n",
    "\n",
    "    soup = BeautifulSoup(article_text, \"html.parser\")\n",
    "    text = \"\"\n",
    "    tags = soup.findAll(\"p\")\n",
    "    for t in tags:\n",
    "        text = text + t.text\n",
    "    #only take contents of <p> tags\n",
    "    #this ensures we only take the main text while discarding extraneous material \n",
    "    #(references etc.)\n",
    "        \n",
    "    sentences = re.split('\\. |\\.\\\\n|\\.\\[\\d+\\]', text)\n",
    "    #split on: period followed by space | period followed by line break | period followed by citation \n",
    "\n",
    "    sentences = [s.split() for s in sentences]\n",
    "    #split on whitespace\n",
    "    sentences = [s for s in sentences if len(s) > 2 and len(s) < 50]\n",
    "    #remove unsually short or long sentences\n",
    "\n",
    "    remove_citations = lambda s : [t for t in s if \"[\" not in t and \"]\" not in t]\n",
    "    sentences = [remove_citations(s) for s in sentences]\n",
    "    \n",
    "    remove_listens = lambda s : [t for t in s if \"/\" not in t and not t==\"(listen)\"]\n",
    "    sentences = [remove_listens(s) for s in sentences]\n",
    "    \n",
    "    #remove other extraneous punctuation?\n",
    "    \n",
    "    sentences = [\" \".join(s) for s in sentences]\n",
    "    \n",
    "    return sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load texts from database into list of dicts (database_dict_list)\n",
    "database_dict_list = []\n",
    "sqlstr = 'SELECT url, text FROM Negative'\n",
    "for row in cur.execute(sqlstr):\n",
    "    entry = {}\n",
    "    entry['url'] = row[0]\n",
    "    entry['text'] = row[1]\n",
    "    database_dict_list.append(entry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [d['text'] for d in database_dict_list]\n",
    "training_negative_cases = []\n",
    "for t in texts: training_negative_cases = training_negative_cases + clean_wikipedia_text(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2216"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For E-ZPass users, sensors detect their transponders wirelessly\n",
      "\n",
      "The Korg OASYS, the Korg KRONOS, the Yamaha Motif XF music Yamaha Yamaha synthesizers, Yamaha Motif-Rack XS tone generator module, and Roland RD-700GX digital piano also run Linux\n",
      "\n",
      "Separately, the Board of Estimate voted to create an authority to impose toll charges on both crossings\n",
      "\n",
      "Larry Ellison's Oracle Database (or more simply, Oracle) started from a different chain, based on IBM's papers on System R\n",
      "\n",
      "Many other open-source software projects contribute to Linux systems\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#print a few cases at random\n",
    "_=[print(s + \"\\n\") for s in np.array(training_negative_cases)[np.random.permutation(len(training_positive_cases))[0:5]]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Train the classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I want to develop a classifier which takes sentences or parts of sentences as input and returns an estimated probability that the input describes an instance of failure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_pretrained_embeddings_path = '/users/cstoneki/Documents/analysis/general_resources/glove.6B/glove.6B.300d.txt'\n",
    "#glove_pretrained_embeddings_path = '/mnt/glove.6B.50d.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load GloVe data\n",
    "#this can take a bit of time, especially for the higher-dimensional datasets (such as 300d)\n",
    "#so report progress\n",
    "\n",
    "import numpy as np\n",
    "with open(glove_pretrained_embeddings_path) as f:\n",
    "    n_entries = 0\n",
    "    d = 0\n",
    "    \n",
    "    for k, line in enumerate(f.readlines()):\n",
    "        n_entries = k + 1\n",
    "        #the first entry is \"the\", it is well formatted\n",
    "        if(k==0): d = len(line.split()) - 1\n",
    "    glove_data = np.zeros([d, n_entries])\n",
    "    words = []\n",
    "    #store each entry (word) as column\n",
    "    print('Found %d words in glove dataset'%n_entries)\n",
    "    f.seek(0)\n",
    "    for k, line in enumerate(f.readlines()):\n",
    "        lst = line.split()\n",
    "        words.append(lst[0])\n",
    "        vals = np.array([float(s) for s in lst[1:]])\n",
    "        glove_data[:,k] = vals\n",
    "        if(k % 10000==0):\n",
    "            print('Words loaded : %06d '%k)\n",
    "    print('Finished loading data')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_df = pd.DataFrame(glove_data, columns=words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Group data into sentences, not single words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute word embeddings from a piece of text\n",
    "#keep the grouping of words into sentences\n",
    "\n",
    "#return: sentence_embeddings = list of embedded sentences\n",
    "#           if output == \"index\":\n",
    "#           each embedded sentence is a N x 1 vector of indices into embedding matrix\n",
    "#           where N = number of tokens in the sentence\n",
    "#           if output == \"full_embedding\"\n",
    "#           each embedded sentence is a M x N matrix\n",
    "#           where M is embedding space dimension, N is number of tokens in the sentence\n",
    "#        valid_sentences = list of raw text (string) of embedded sentences\n",
    "def embed_grouped_by_sentence(glove_df, text, output=\"index\"):\n",
    "    sentences = [s for s in text.split(\".\") if len(s.split()) > 1]\n",
    "    sentence_embeddings = []\n",
    "    valid_sentences = []\n",
    "    for s in sentences:\n",
    "        single_word_embeddings = []\n",
    "        words = s.split()\n",
    "        for w in words:\n",
    "            if(w in glove_df.columns):\n",
    "                if(output==\"full_embedding\"):\n",
    "                    single_word_embeddings.append(glove_df[w].values[:,np.newaxis])\n",
    "                elif(output==\"index\"):\n",
    "                    single_word_embeddings.append(glove_df.columns.get_loc(w))\n",
    "        if(len(single_word_embeddings) > 1):\n",
    "            if(output==\"full_embedding\"):\n",
    "                sentence_embeddings.append(np.concatenate(single_word_embeddings, axis=1))\n",
    "            elif(output==\"index\"):\n",
    "                sentence_embeddings.append(np.array(single_word_embeddings))\n",
    "            valid_sentences.append(s)\n",
    "    return (sentence_embeddings, valid_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_df.columns.get_loc('the')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_df.values[2,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Scrape data, store in SQL database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable}  -m pip install keras\n",
    "#!{sys.executable} -m pip install beautifulsoup4\n",
    "#!{sys.executable} -m pip install requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_urls = {'failory failure':'https://www.failory.com/interview-failure',\n",
    "             'failory success':'https://www.failory.com/interview-success'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_report(cur,  url, text, tags):\n",
    "    cur.execute('SELECT text FROM Startups WHERE url = ?', (url,))\n",
    "    #? -> avoid SQL injection\n",
    "    row = cur.fetchone()\n",
    "    if(row is None):\n",
    "        cur.execute('''INSERT INTO Startups (url, text, tags)\n",
    "            VALUES (?, ?, ?)''', (url, text, tags))\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = sqlite3.connect('startups_03.sqlite')\n",
    "cur = conn.cursor()\n",
    "cur.execute('CREATE TABLE IF NOT EXISTS Startups (url TEXT, text TEXT, tags TEXT) ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tags, url in main_urls.items():\n",
    "    print('\\ncollecting articles from '+url+'\\n')\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    a_tags = soup.findAll('a')\n",
    "    sub_urls = []\n",
    "    for i in range(len(a_tags)):\n",
    "        try:\n",
    "            if(a_tags[i][\"class\"][0] =='card-for-interviews-title'):\n",
    "                sub_urls.append(a_tags[i][\"href\"])\n",
    "        except:\n",
    "            continue\n",
    "    for sub_url in sub_urls:\n",
    "        full_url = 'https://www.failory.com' + sub_url\n",
    "        article  = requests.get(full_url)\n",
    "        print('downloaded '+full_url)\n",
    "        time.sleep(1)\n",
    "        \n",
    "        insert_report(cur, full_url, article.text, tags)\n",
    "        conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check contents of database\n",
    "#by retrieving small text fields, not full text\n",
    "sqlstr = 'SELECT url, tags FROM Startups'\n",
    "database_dict = {}\n",
    "for row in cur.execute(sqlstr):\n",
    "    database_dict[str(row[0])] = [row[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print text of first article\n",
    "sqlstr = 'SELECT url, text, tags FROM Startups'\n",
    "for k, row in enumerate(cur.execute(sqlstr)):\n",
    "    if(k > 0): break\n",
    "    soup = BeautifulSoup(row[1], \"html.parser\")\n",
    "    print(soup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Process HTML data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to figure out how to extract the text of the article from the mess of HTML. We need to strip out all of the ads and repeated quotes. One key part will be extracting the interviewer's questions, and the response that follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#failory has tags at the start of each article\n",
    "#these are: location, area, failure cause #1, failure cause #2\n",
    "#these are obviously extremely useful, so we want to extract them\n",
    "#try to find location\n",
    "def get_failory_tags(text):\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    article_tags = []\n",
    "    div_tags = soup.findAll('div')\n",
    "    for i in range(len(div_tags)):\n",
    "        try:\n",
    "            if(div_tags[i][\"class\"][0] ==\"secondary-tag-interview\"):\n",
    "                if(div_tags[i].text):\n",
    "                    article_tags.append(div_tags[i].text)\n",
    "        except:\n",
    "            continue\n",
    "    return article_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "database_dict_list = []\n",
    "sqlstr = 'SELECT url, text, tags FROM Startups'\n",
    "for row in cur.execute(sqlstr):\n",
    "    entry = {}\n",
    "    entry['url'] = row[0]\n",
    "    entry['text'] = row[1]\n",
    "    entry['tags'] = row[2]\n",
    "    database_dict_list.append(entry)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for entry in database_dict_list:\n",
    "    if('failory' in entry['tags']):\n",
    "        #the following will only work for failory articles\n",
    "        #so check because we may have non-failory articles in database later\n",
    "        entry['failory_tags'] = get_failory_tags(entry['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "database_dict_list[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_list = [entry['failory_tags'][0] for entry in database_dict_list if 'failory_tags' in entry]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(set(country_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from geopy.geocoders import Nominatim\n",
    "geolocator = Nominatim(user_agent=\"startup-analysis\")\n",
    "\n",
    "#get coordinates for each country\n",
    "locations = []\n",
    "scales = []\n",
    "for c in list(set(country_list)):\n",
    "    \n",
    "    location = geolocator.geocode(c)\n",
    "    scales.append(int(np.sum(np.array(country_list)==c)))\n",
    "    entry = {}\n",
    "    entry['latitude'] = location.latitude\n",
    "    entry['longitude'] = location.longitude\n",
    "    locations.append(entry)\n",
    "locations = pd.DataFrame(locations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scales_to_plot = [int(np.floor(1.5*np.sqrt(s) + 0.5)) for s in scales]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import gmaps\n",
    "gmaps.configure(api_key=googlemaps_api_key)\n",
    "coordinates = (30, 0)\n",
    "fig = gmaps.figure(center=coordinates, zoom_level=2, layout={'width': '1000px', 'height': '600px'})\n",
    "\n",
    "\n",
    "startup_layer = gmaps.symbol_layer(\n",
    "    locations, fill_color='blue', stroke_color='blue', scale = scales_to_plot\n",
    ")\n",
    "fig.add_layer(startup_layer)\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#display a previously saved image\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "plt.figure(figsize=(20,10))\n",
    "img=mpimg.imread('map.png')\n",
    "imgplot = plt.imshow(img)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#now try to extract text of interest\n",
    "def get_questions_responses(text):\n",
    "\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    tags = soup.findAll(['h4', 'p'])\n",
    "    tags_clean = []\n",
    "    for i in range(len(tags)):\n",
    "        try:\n",
    "            if(tags[i][\"class\"][0]):\n",
    "                continue\n",
    "        except:\n",
    "            tags_clean.append(tags[i])\n",
    "        \n",
    "    questions = []\n",
    "    responses = []\n",
    "    current_text = []\n",
    "    current_question = \"\"\n",
    "    for i in range(len(tags_clean)):\n",
    "        if(tags_clean[i].name=='h4'):\n",
    "            if(current_question):\n",
    "                questions.append(current_question)\n",
    "                responses.append(\" \".join(current_text))\n",
    "            current_question = tags_clean[i].text\n",
    "            current_text = []\n",
    "        else:\n",
    "            current_text.append(tags_clean[i].text)\n",
    "            \n",
    "    return (questions, responses)\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for entry in database_dict_list:\n",
    "    q,r = get_questions_responses(entry['text'])\n",
    "    entry['questions'] = q\n",
    "    entry['responses'] = r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "database_dict_list[0]['questions']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "database_dict_list[-1]['questions']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've spent a bit of computational time developing database_dict_list, so save it as json. Use json rather than sql a) because it's much easier to handle fields that are lists of variable length and b) because we are not growing the data entry-by-entry, but dumping a single finished database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('data.json', 'w') as fp:\n",
    "    json.dump(database_dict_list, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start here if loading from json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('data.json', 'r') as fp:\n",
    "    data = json.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#determine which tags failory has used\n",
    "failure_reason_tags = []\n",
    "area_tags = []\n",
    "\n",
    "for d in data:\n",
    "    \n",
    "    if('failure' in d['tags'].split()):\n",
    "        failure_reason_tags = failure_reason_tags + d['failory_tags'][2:3]\n",
    "        \n",
    "failure_reason_tags = np.array(failure_reason_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#failure_reason_tags\n",
    "\n",
    "unique_failure_reasons = np.unique(failure_reason_tags)\n",
    "counts = np.array([np.sum(failure_reason_tags==r) for r in unique_failure_reasons])\n",
    "order = np.argsort(counts)\n",
    "counts = counts[order]\n",
    "labels = unique_failure_reasons[order]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vals = np.arange(len(counts))\n",
    "plt.barh(vals,counts)\n",
    "plt.yticks(vals, [lab + \" \" for lab in labels])\n",
    "plt.title('Failure reasons, according to Failory\\n')\n",
    "plt.xlabel('Number of cases')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Now start to work with word embeddings\n",
    "\n",
    "Use pretrained GloVe embeddings from https://nlp.stanford.edu/projects/glove/\n",
    "\n",
    "Specifically, the 6B dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contrast_text = \"\"\"\n",
    "In February 2015, the company wrote that around 10,000 new daily active users were signing up each week,\n",
    "and had more than 135,000 paying customers spread across 60,000 teams.\n",
    "Slack offers many IRC-like features, including persistent chat rooms (channels) organized by topic, private groups, and direct messaging.\n",
    "Zulip was originally developed as proprietary software by a startup called Zulip, Inc., based in Cambridge, Massachusetts.\n",
    "In 2014, while in private beta, the company was acquired by Dropbox.\n",
    "In September 2015, Dropbox open-sourced it under the Apache License.\n",
    "Today, it is a leading open source alternative to Slack or HipChat, with over 29,000 commits contributed by 450 people.\n",
    "Microsoft Excel is a spreadsheet developed by Microsoft for Windows, macOS, Android and iOS.\n",
    "It features calculation, graphing tools, pivot tables, and a macro programming language called Visual Basic for Applications.\n",
    "It has been a very widely applied spreadsheet for these platforms, especially since version 5 in 1993, and it has replaced Lotus 1-2-3 as the industry standard for spreadsheets.\n",
    "Excel forms part of the Microsoft Office suite of software.\n",
    "Microsoft Excel has the basic features of all spreadsheets, using a grid of cells arranged in numbered rows and letter-named columns to organize data manipulations like arithmetic operations.\n",
    "It has a battery of supplied functions to answer statistical, engineering and financial needs.\n",
    "In addition, it can display data as line graphs, histograms and charts, and with a very limited three-dimensional graphical display.\n",
    "It allows sectioning of data to view its dependencies on various factors for different perspectives (using pivot tables and the scenario manager).\n",
    "It has a programming aspect, Visual Basic for Applications, allowing the user to employ a wide variety of numerical methods, for example, for solving differential equations of mathematical physics, and then reporting the results back to the spreadsheet.\n",
    "It also has a variety of interactive features allowing user interfaces that can completely hide the spreadsheet from the user, so the spreadsheet presents itself as a so-called application,\n",
    "or decision support system (DSS), via a custom-designed user interface, for example, a stock analyzer, or in general, as a design tool that asks the user questions and provides answers and reports.\n",
    "In a more elaborate realization, an Excel application can automatically poll external databases and measuring instruments using an update schedule,analyze the results, make a Word report or PowerPoint slide show, and e-mail these presentations on a regular basis to a list of participants.\n",
    "Excel was not designed to be used as a database.\n",
    "Microsoft allows for a number of optional command-line switches to control the manner in which Excel starts.\n",
    "The Windows version of Excel supports programming through Microsoft's Visual Basic for Applications (VBA), which is a dialect of Visual Basic. Programming with VBA allows spreadsheet manipulation that is awkward or impossible with standard spreadsheet techniques.\n",
    "Programmers may write code directly using the Visual Basic Editor (VBE), which includes a window for writing code, debugging code, and code module organization environment. The user can implement numerical methods as well as automating tasks such as formatting or data organization in VBA and guide the calculation using any desired intermediate results reported back to the spreadsheet.\n",
    "\n",
    "VBA was removed from Mac Excel 2008, as the developers did not believe that a timely release would allow porting the VBA engine natively to Mac OS X.\n",
    "VBA was restored in the next version, Mac Excel 2011, although the build lacks support for ActiveX objects, impacting some high level developer tools.\n",
    "A common and easy way to generate VBA code is by using the Macro Recorder.\n",
    "The Macro Recorder records actions of the user and generates VBA code in the form of a macro.\n",
    "These actions can then be repeated automatically by running the macro.\n",
    "The macros can also be linked to different trigger types like keyboard shortcuts, a command button or a graphic.\n",
    "The actions in the macro can be executed from these trigger types or from the generic toolbar options.\n",
    "The VBA code of the macro can also be edited in the VBE.\n",
    "Certain features such as loop functions and screen prompt by their own properties, and some graphical display items, cannot be recorded but must be entered into the VBA module directly by the programmer.\n",
    "Advanced users can employ user prompts to create an interactive program, or react to events such as sheets being loaded or changed.\n",
    "\n",
    "Macro Recorded code may not be compatible with Excel versions.\n",
    "Some code that is used in Excel 2010 cannot be used in Excel 2003.\n",
    "Making a Macro that changes the cell colours and making changes to other aspects of cells may not be backward compatible.\n",
    "\n",
    "VBA code interacts with the spreadsheet through the Excel Object Model, a vocabulary identifying spreadsheet objects, and a set of supplied functions or methods that enable reading and writing to the spreadsheet and interaction with its users (for example, through custom toolbars or command bars and message boxes).\n",
    "User-created VBA subroutines execute these actions and operate like macros generated using the macro recorder, but are more flexible and efficient.\n",
    "From its first version Excel supported end user programming of macros (automation of repetitive tasks) and user defined functions (extension of Excel's built-in function library).\n",
    "In early versions of Excel these programs were written in a macro language whose statements had formula syntax and resided in the cells of special purpose macro sheets (stored with file extension .XLM in Windows.)\n",
    "XLM was the default macro language for Excel through Excel 4.0. Beginning with version 5.0 Excel recorded macros in VBA by default but with version 5.0 XLM recording was still allowed as an option.\n",
    "After version 5.0 that option was discontinued.\n",
    "All versions of Excel, including Excel 2010 are capable of running an XLM macro, though Microsoft discourages their use.\n",
    "Excel supports charts, graphs, or histograms generated from specified groups of cells. The generated graphic component can either be embedded within the current sheet, or added as a separate object.\n",
    "These displays are dynamically updated if the content of cells change. For example, suppose that the important design requirements are displayed visually; then, in response to a user's change in trial values for parameters, the curves describing the design change shape, and their points of intersection shift, assisting the selection of the best design.\n",
    "Microsoft originally marketed a spreadsheet program called Multiplan in 1982. Multiplan became very popular on CP/M systems, but on MS-DOS systems it lost popularity to Lotus 1-2-3. Microsoft released the first version of Excel for the Macintosh on September 30, 1985, and the first Windows version was 2.05 (to synchronize with the Macintosh version 2.2) in November 1987. Lotus was slow to bring 1-2-3 to Windows and by the early 1990s Excel had started to outsell 1-2-3 and helped Microsoft achieve its position as a leading PC software developer. This accomplishment solidified Microsoft as a valid competitor and showed its future of developing GUI software. Microsoft maintained its advantage with regular new releases, every two years or so.\n",
    "Instagram (also known as IG or Insta) is a photo and video-sharing social networking service owned by Facebook, Inc.\n",
    "It was created by Kevin Systrom and Mike Krieger, and launched in October 2010 exclusively on iOS.\n",
    "A version for Android devices was released a year and half later, in April 2012, followed by a feature-limited website interface in November 2012, and apps for Windows 10 Mobile and Windows 10 in April 2016 and October 2016 respectively.\n",
    "The app allows users to upload photos and videos to the service, which can be edited with various filters, and organized with tags and location information.\n",
    "An account's posts can be shared publicly or with pre-approved followers. Users can browse other users' content by tags and locations, and view trending content. Users can like photos, and follow other users to add their content to a feed.\n",
    "\n",
    "The service was originally distinguished by only allowing content to be framed in a square (1:1) aspect ratio, but these restrictions were eased in 2015. The service also added messaging features, the ability to include multiple images or videos in a single post, as well as Stories—similar to its main competitor Snapchat—which allows users to post photos and videos to a sequential feed, with each post accessible by others for 24 hours each. As of January 2019, the Stories feature is being used by 500 million users daily.\n",
    "\n",
    "After its launch in 2010, Instagram rapidly gained popularity, with one million registered users in two months, 10 million in a year, and 1 billion as of May 2019. In April 2012, Facebook acquired the service for approximately US$1 billion in cash and stock. As of October 2015, over 40 billion photos had been uploaded to the service. Although praised for its influence, Instagram has been the subject of criticism, most notably for policy and interface changes, allegations of censorship, and illegal or improper content uploaded by users.\n",
    "\n",
    "As of January 14, 2019, the most liked photo on Instagram is a picture of an egg, posted by the account @world_record_egg, created with the sole purpose of surpassing the previous record of 18 million likes on a Kylie Jenner post. The picture currently has over 53 million likes.\n",
    "\n",
    "Instagram began development in San Francisco, when Kevin Systrom and Mike Krieger chose to focus their multi-featured HTML5 check-in project, Burbn, on mobile photography. As Krieger reasoned, Burbn became too similar to Foursquare, and both realized that it had gone too far. Burbn was then pivoted to become more focused on photo-sharing. The word Instagram is a portmanteau of instant camera and telegram.\n",
    "\n",
    "In December 2013, Instagram announced Instagram Direct, a feature that lets users interact through private messaging. Users who follow each other can send private messages with photos and videos, in contrast to the public-only requirement that was previously in place. When users receive a private message from someone they don't follow, the message is marked as pending and the user must accept to see it. Users can send a photo to a maximum of 15 people. The feature received a major update in September 2015, adding conversation threading and making it possible for users to share locations, hashtag pages, and profiles through private messages directly from the news feed. Additionally, users can now reply to private messages with text, emoji or by clicking on a heart icon. A camera inside Direct lets users take a photo and send it to the recipient without leaving the conversation.A new update in November 2016 let users make their private messages disappear after being viewed by the recipient, with the sender receiving a notification if the recipient takes a screenshot. In April 2017, Instagram redesigned Direct to combine all private messages, both permanent and ephemeral, into the same message threads.In May, Instagram made it possible to send website links in messages, and also added support for sending photos in their original portrait or landscape orientation without cropping.\n",
    "\n",
    "Hudson Yards is a real estate development in the Chelsea and Hudson Yards neighborhoods of Manhattan, New York City. It is the largest private real estate development in the United States by area. Upon completion, 13 of the 16 planned structures on the West Side of Midtown South would sit on a platform built over the West Side Yard, a storage yard for Long Island Rail Road trains. The first of its two phases, opened in 2019, comprises a public green space and eight structures that contain residences, a hotel, office buildings, a mall, and a cultural facility. The second phase, on which construction has not started yet, will include residential space, an office building, and a school.\n",
    "\n",
    "A suspension bridge is a type of bridge in which the deck (the load-bearing portion) is hung below suspension cables on vertical suspenders. The first modern examples of this type of bridge were built in the early 1800s. Simple suspension bridges, which lack vertical suspenders, have a long history in many mountainous parts of the world.\n",
    "\n",
    "This type of bridge has cables suspended between towers, plus vertical suspender cables that carry the weight of the deck below, upon which traffic crosses. This arrangement allows the deck to be level or to arc upward for additional clearance. Like other suspension bridge types, this type often is constructed without falsework.\n",
    "\n",
    "The suspension cables must be anchored at each end of the bridge, since any load applied to the bridge is transformed into a tension in these main cables. The main cables continue beyond the pillars to deck-level supports, and further continue to connections with anchors in the ground. The roadway is supported by vertical suspender cables or rods, called hangers. In some circumstances, the towers may sit on a bluff or canyon edge where the road may proceed directly to the main span, otherwise the bridge will usually have two smaller spans, running between either pair of pillars and the highway, which may be supported by suspender cables or may use a truss bridge to make this connection. In the latter case there will be very little arc in the outboard main cables.\n",
    "The principles of suspension used on the large scale may also appear in contexts less dramatic than road or rail bridges. Light cable suspension may prove less expensive and seem more elegant for a cycle or footbridge than strong girder supports. An example of this is the Nescio Bridge in the Netherlands.\n",
    "\n",
    "Where such a bridge spans a gap between two buildings, there is no need to construct special towers, as the buildings can anchor the cables. Cable suspension may also be augmented by the inherent stiffness of a structure that has much in common with a tubular bridge.\n",
    "\n",
    "US 66 served as a primary route for those who migrated west, especially during the Dust Bowl of the 1930s, and the road supported the economies of the communities through which it passed. People doing business along the route became prosperous due to the growing popularity of the highway, and those same people later fought to keep the highway alive in the face of the growing threat of being bypassed by the new Interstate Highway System.\n",
    "\n",
    "US 66 underwent many improvements and realignments over its lifetime, but was officially removed from the United States Highway System in 1985 after it had been replaced in its entirety by segments of the Interstate Highway System. Portions of the road that passed through Illinois, Missouri, New Mexico, and Arizona have been communally designated a National Scenic Byway of the name Historic Route 66, returning the name to some maps. Several states have adopted significant bypassed sections of the former US 66 into their state road networks as State Route 66. The corridor is also being redeveloped into U.S. Bicycle Route 66, a part of the United States Bicycle Route System that was developed in the 2010s.\n",
    "\n",
    "Computer software, or simply software, is a collection of data or computer instructions that tell the computer how to work. This is in contrast to physical hardware, from which the system is built and actually performs the work. In computer science and software engineering, computer software is all information processed by computer systems, programs and data. Computer software includes computer programs, libraries and related non-executable data, such as online documentation or digital media. Computer hardware and software require each other and neither can be realistically used on its own.\n",
    "\n",
    "At the lowest programming level, executable code consists of machine language instructions supported by an individual processor—typically a central processing unit (CPU) or a graphics processing unit (GPU). A machine language consists of groups of binary values signifying processor instructions that change the state of the computer from its preceding state. For example, an instruction may change the value stored in a particular storage location in the computer—an effect that is not directly observable to the user. An instruction may also invoke one of many input or output operations, for example displaying some text on a computer screen; causing state changes which should be visible to the user. The processor executes the instructions in the order they are provided, unless it is instructed to \"jump\" to a different instruction, or is interrupted by the operating system. As of 2015, most personal computers, smartphone devices and servers have processors with multiple execution units or multiple processors performing computation together, and computing has become a much more concurrent activity than in the past.\n",
    "\n",
    "The majority of software is written in high-level programming languages. They are easier and more efficient for programmers because they are closer to natural languages than machine languages. High-level languages are translated into machine language using a compiler or an interpreter or a combination of the two. Software may also be written in a low-level assembly language, which has strong correspondence to the computer's machine language instructions and is translated into machine language using an assembler.\n",
    "\n",
    "An outline (algorithm) for what would have been the first piece of software was written by Ada Lovelace in the 19th century, for the planned Analytical Engine. She created proofs to show how the engine would calculate Bernoulli Numbers. Because of the proofs and the algorithm, she is considered the first computer programmer.\n",
    "\n",
    "The first theory about software—prior to creation of computers as we know them today—was proposed by Alan Turing in his 1935 essay On Computable Numbers, with an Application to the Entscheidungsproblem (decision problem).\n",
    "\n",
    "This eventually led to the creation of the academic fields of computer science and software engineering; Both fields study software and its creation. Computer science is the theoretical study of computer and software (Turing's essay is an example of computer science), whereas software engineering is the application of engineering and development of software.\n",
    "\n",
    "However, prior to 1946, software was not yet the programs stored in the memory of stored-program digital computers, as we now understand it. The first electronic computing devices were instead rewired in order to \"reprogram\" them.\n",
    "\n",
    "In 2000, Fred Shapiro, a librarian at the Yale Law School, published a letter revealing that John Wilder Tukey's 1958 paper The Teaching of Concrete Mathematics contained the earliest known usage of the term \"software\" found in a search of JSTOR's electronic archives, predating the OED's citation by two years.This led many to credit Tukey with coining the term, particularly in obituaries published that same year,although Tukey never claimed credit for any such coinage. In 1995, Paul Niquette claimed he had originally coined the term in October 1953, although he could not find any documents supporting his claim.The earliest known publication of the term \"software\" in an engineering context was in August 1953 by Richard R. Carhart, in a Rand Corporation Research Memorandum.\n",
    "\n",
    "Programming tools are also software in the form of programs or applications that software developers (also known as programmers, coders, hackers or software engineers) use to create, debug, maintain (i.e. improve or fix), or otherwise support software.\n",
    "\n",
    "Software is written in one or more programming languages; there are many programming languages in existence, and each has at least one implementation, each of which consists of its own set of programming tools. These tools may be relatively self-contained programs such as compilers, debuggers, interpreters, linkers, and text editors, that can be combined together to accomplish a task; or they may form an integrated development environment (IDE), which combines much or all of the functionality of such self-contained tools. IDEs may do this by either invoking the relevant individual tools or by re-implementing their functionality in a new way. An IDE can make it easier to do specific tasks, such as searching in files in a particular project. Many programming language implementations provide the option of using both individual tools or an IDE. \n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from training_data_failure_sentences import failure_sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "failure_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = []\n",
    "sentences = []\n",
    "labels = []\n",
    "for s,_ in failure_sentences.items():\n",
    "    e, s = embed_grouped_by_sentence(glove_df, s)\n",
    "    embeddings = embeddings + e\n",
    "    sentences = sentences + s\n",
    "    labels = labels + [1]*len(s)\n",
    "    \n",
    "e, s = embed_grouped_by_sentence(glove_df, contrast_text)\n",
    "embeddings = embeddings + e\n",
    "sentences = sentences + s\n",
    "labels = labels + [0]*len(s)\n",
    "labels = np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.sum(np.array([x==0 for x in labels])))\n",
    "print(np.sum(np.array([x==1 for x in labels])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = np.max(np.array([len(e) for e in embeddings]))\n",
    "embedding_matrix = np.zeros([len(embeddings), max_len])\n",
    "for k,e in enumerate(embeddings):\n",
    "    embedding_matrix[k,0:len(e)] = e\n",
    "labels = np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Input, Dropout, LSTM, Activation\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "from keras.initializers import glorot_uniform\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The approach here is to pass Keras the vectors of word indices (each sentence is one vector).\n",
    "A pretrained embedding layer is used to convert word indices into full word embedding vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretrained_embedding_layer(embedding_df):\n",
    "\n",
    "    embedding_layer = Embedding(embedding_df.shape[1] + 1, embedding_df.shape[0], trainable=False)\n",
    "    embedding_layer.build((None,))\n",
    "    embed_matrix = np.transpose(embedding_df.values)\n",
    "    embed_matrix = np.concatenate([embed_matrix, np.zeros([1, embed_matrix.shape[1]])], axis=0)\n",
    "    embedding_layer.set_weights([embed_matrix])\n",
    "    return embedding_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LSTM_graph(max_len, embedding_df):\n",
    "    index_vectors = Input(shape=(max_len,) , dtype='int32')\n",
    "    embedding_layer = pretrained_embedding_layer(embedding_df)\n",
    "    embeddings = embedding_layer(index_vectors)\n",
    "    X = LSTM(units=128, return_sequences=True)(embeddings)\n",
    "    X = Dropout(rate=0.5)(X)\n",
    "    # Propagate X trough another LSTM layer with 128-dimensional hidden state\n",
    "    # Here the returned output is a single hidden state, not a batch of sequences.\n",
    "    X = LSTM(units=128, return_sequences=False)(X)\n",
    "    # Add dropout with a probability of 0.5\n",
    "    X = Dropout(rate=0.5)(X)\n",
    "    # Single sigmoid output unit\n",
    "    X = Dense(units=1, activation='sigmoid')(X)\n",
    "    \n",
    "    # Create Model instance which converts sentence_indices into X.\n",
    "    model = Model(inputs=index_vectors, outputs=X)\n",
    "    \n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTM_graph(max_len, glove_df)\n",
    "model.summary()\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training LSTM on fold 1 / 5 :\n",
      "\n",
      "Epoch 1/50\n",
      "509/509 [==============================] - 4s 7ms/step - loss: 0.5842 - acc: 0.7387\n",
      "Epoch 2/50\n",
      "509/509 [==============================] - 1s 3ms/step - loss: 0.5753 - acc: 0.7603\n",
      "Epoch 3/50\n",
      "509/509 [==============================] - 1s 3ms/step - loss: 0.5628 - acc: 0.7623\n",
      "Epoch 4/50\n",
      "509/509 [==============================] - 1s 3ms/step - loss: 0.5544 - acc: 0.7623\n",
      "Epoch 5/50\n",
      "509/509 [==============================] - 1s 3ms/step - loss: 0.5675 - acc: 0.7623\n",
      "Epoch 6/50\n",
      "509/509 [==============================] - 1s 3ms/step - loss: 0.5539 - acc: 0.7642\n",
      "Epoch 7/50\n",
      "509/509 [==============================] - 1s 3ms/step - loss: 0.5530 - acc: 0.7642\n",
      "Epoch 8/50\n",
      "509/509 [==============================] - 1s 3ms/step - loss: 0.5630 - acc: 0.7642\n",
      "Epoch 9/50\n",
      "509/509 [==============================] - 1s 3ms/step - loss: 0.5491 - acc: 0.7642\n",
      "Epoch 10/50\n",
      "509/509 [==============================] - 1s 3ms/step - loss: 0.5613 - acc: 0.7642\n",
      "Epoch 11/50\n",
      "509/509 [==============================] - 2s 3ms/step - loss: 0.5613 - acc: 0.7642\n",
      "Epoch 12/50\n",
      "509/509 [==============================] - 2s 3ms/step - loss: 0.5582 - acc: 0.7642\n",
      "Epoch 13/50\n",
      "509/509 [==============================] - 2s 3ms/step - loss: 0.5543 - acc: 0.7642\n",
      "Epoch 14/50\n",
      "509/509 [==============================] - 2s 3ms/step - loss: 0.5547 - acc: 0.7642\n",
      "Epoch 15/50\n",
      "509/509 [==============================] - 2s 3ms/step - loss: 0.5651 - acc: 0.7741\n",
      "Epoch 16/50\n",
      "509/509 [==============================] - 2s 3ms/step - loss: 0.5514 - acc: 0.7662\n",
      "Epoch 17/50\n",
      "509/509 [==============================] - 2s 3ms/step - loss: 0.5195 - acc: 0.7957\n",
      "Epoch 18/50\n",
      "509/509 [==============================] - 2s 3ms/step - loss: 0.3909 - acc: 0.8762\n",
      "Epoch 19/50\n",
      "509/509 [==============================] - 2s 3ms/step - loss: 0.4686 - acc: 0.8350\n",
      "Epoch 20/50\n",
      "509/509 [==============================] - 2s 3ms/step - loss: 0.5441 - acc: 0.7741\n",
      "Epoch 21/50\n",
      "509/509 [==============================] - 2s 3ms/step - loss: 0.5352 - acc: 0.7741\n",
      "Epoch 22/50\n",
      "509/509 [==============================] - 2s 3ms/step - loss: 0.5029 - acc: 0.7741\n",
      "Epoch 23/50\n",
      "509/509 [==============================] - 2s 3ms/step - loss: 0.4300 - acc: 0.7976\n",
      "Epoch 24/50\n",
      "509/509 [==============================] - 2s 3ms/step - loss: 0.2914 - acc: 0.8939\n",
      "Epoch 25/50\n",
      "509/509 [==============================] - 2s 3ms/step - loss: 0.2130 - acc: 0.9293\n",
      "Epoch 26/50\n",
      "509/509 [==============================] - 2s 3ms/step - loss: 0.2163 - acc: 0.9371\n",
      "Epoch 27/50\n",
      "509/509 [==============================] - 2s 3ms/step - loss: 0.1614 - acc: 0.9587\n",
      "Epoch 28/50\n",
      "509/509 [==============================] - 2s 3ms/step - loss: 0.1455 - acc: 0.9666\n",
      "Epoch 29/50\n",
      "509/509 [==============================] - 2s 3ms/step - loss: 0.1105 - acc: 0.9725\n",
      "Epoch 30/50\n",
      "509/509 [==============================] - 2s 3ms/step - loss: 0.0872 - acc: 0.9784\n",
      "Epoch 31/50\n",
      "509/509 [==============================] - 2s 3ms/step - loss: 0.0675 - acc: 0.9843\n",
      "Epoch 32/50\n",
      "509/509 [==============================] - 2s 3ms/step - loss: 0.0525 - acc: 0.9902\n",
      "Epoch 33/50\n",
      "509/509 [==============================] - 2s 3ms/step - loss: 0.0818 - acc: 0.9804\n",
      "Epoch 34/50\n",
      "509/509 [==============================] - 2s 3ms/step - loss: 0.0555 - acc: 0.9862\n",
      "Epoch 35/50\n",
      "509/509 [==============================] - 2s 3ms/step - loss: 0.0631 - acc: 0.9764\n",
      "Epoch 36/50\n",
      "509/509 [==============================] - 2s 3ms/step - loss: 0.0450 - acc: 0.9941\n",
      "Epoch 37/50\n",
      "509/509 [==============================] - 2s 3ms/step - loss: 0.0361 - acc: 0.9941\n",
      "Epoch 38/50\n",
      "509/509 [==============================] - 2s 3ms/step - loss: 0.0372 - acc: 0.9921\n",
      "Epoch 39/50\n",
      "509/509 [==============================] - 2s 3ms/step - loss: 0.0359 - acc: 0.9941\n",
      "Epoch 40/50\n",
      "509/509 [==============================] - 2s 3ms/step - loss: 0.0373 - acc: 0.9941\n",
      "Epoch 41/50\n",
      "509/509 [==============================] - 2s 3ms/step - loss: 0.0345 - acc: 0.9941\n",
      "Epoch 42/50\n",
      "509/509 [==============================] - 2s 3ms/step - loss: 0.0357 - acc: 0.9941\n",
      "Epoch 43/50\n",
      "509/509 [==============================] - 2s 3ms/step - loss: 0.0354 - acc: 0.9941\n",
      "Epoch 44/50\n",
      "509/509 [==============================] - 2s 3ms/step - loss: 0.0380 - acc: 0.9941\n",
      "Epoch 45/50\n",
      "509/509 [==============================] - 2s 3ms/step - loss: 0.0305 - acc: 0.9941\n",
      "Epoch 46/50\n",
      "509/509 [==============================] - 2s 3ms/step - loss: 0.0337 - acc: 0.9941\n",
      "Epoch 47/50\n",
      "509/509 [==============================] - 2s 3ms/step - loss: 0.0305 - acc: 0.9961\n",
      "Epoch 48/50\n",
      "509/509 [==============================] - 2s 3ms/step - loss: 0.0266 - acc: 0.9961\n",
      "Epoch 49/50\n",
      "509/509 [==============================] - 2s 3ms/step - loss: 0.0266 - acc: 0.9961\n",
      "Epoch 50/50\n",
      "509/509 [==============================] - 2s 3ms/step - loss: 0.0254 - acc: 0.9961\n",
      "\n",
      "Training LSTM on fold 2 / 5 :\n",
      "\n",
      "Epoch 1/50\n",
      "509/509 [==============================] - 4s 8ms/step - loss: 0.5981 - acc: 0.7564\n",
      "Epoch 2/50\n",
      "509/509 [==============================] - 1s 3ms/step - loss: 0.5643 - acc: 0.7603\n",
      "Epoch 3/50\n",
      "509/509 [==============================] - 2s 3ms/step - loss: 0.5671 - acc: 0.7603\n",
      "Epoch 4/50\n",
      "509/509 [==============================] - 2s 3ms/step - loss: 0.5712 - acc: 0.7603\n",
      "Epoch 5/50\n",
      "509/509 [==============================] - 2s 3ms/step - loss: 0.5654 - acc: 0.7603\n",
      "Epoch 6/50\n",
      "509/509 [==============================] - 2s 3ms/step - loss: 0.5507 - acc: 0.7603\n",
      "Epoch 7/50\n",
      "509/509 [==============================] - 2s 3ms/step - loss: 0.5570 - acc: 0.7603\n",
      "Epoch 8/50\n",
      "509/509 [==============================] - 2s 3ms/step - loss: 0.5572 - acc: 0.7623\n",
      "Epoch 9/50\n",
      "509/509 [==============================] - 2s 3ms/step - loss: 0.5633 - acc: 0.7623\n",
      "Epoch 10/50\n",
      "509/509 [==============================] - 2s 3ms/step - loss: 0.5550 - acc: 0.7623\n",
      "Epoch 11/50\n",
      "509/509 [==============================] - 2s 3ms/step - loss: 0.5520 - acc: 0.7623\n",
      "Epoch 12/50\n",
      "509/509 [==============================] - 2s 3ms/step - loss: 0.5571 - acc: 0.7623\n",
      "Epoch 13/50\n",
      "509/509 [==============================] - 2s 3ms/step - loss: 0.5465 - acc: 0.7623\n",
      "Epoch 14/50\n",
      "509/509 [==============================] - 2s 3ms/step - loss: 0.5575 - acc: 0.7623\n",
      "Epoch 15/50\n",
      "509/509 [==============================] - 2s 3ms/step - loss: 0.5486 - acc: 0.7642\n",
      "Epoch 16/50\n",
      "509/509 [==============================] - 2s 3ms/step - loss: 0.5482 - acc: 0.7564\n",
      "Epoch 17/50\n",
      "509/509 [==============================] - 2s 3ms/step - loss: 0.5109 - acc: 0.7976\n",
      "Epoch 18/50\n",
      "509/509 [==============================] - 2s 3ms/step - loss: 0.4836 - acc: 0.8153\n",
      "Epoch 19/50\n",
      "509/509 [==============================] - 2s 3ms/step - loss: 0.3357 - acc: 0.8861\n",
      "Epoch 20/50\n",
      "509/509 [==============================] - 2s 3ms/step - loss: 0.4171 - acc: 0.7898\n",
      "Epoch 21/50\n",
      "509/509 [==============================] - 2s 3ms/step - loss: 0.3857 - acc: 0.8153\n",
      "Epoch 22/50\n",
      "509/509 [==============================] - 2s 3ms/step - loss: 0.5303 - acc: 0.7505\n",
      "Epoch 23/50\n",
      "509/509 [==============================] - 2s 3ms/step - loss: 0.5131 - acc: 0.7976\n",
      "Epoch 24/50\n",
      "509/509 [==============================] - 2s 3ms/step - loss: 0.4736 - acc: 0.8035\n",
      "Epoch 25/50\n",
      "509/509 [==============================] - 2s 3ms/step - loss: 0.4267 - acc: 0.8055\n",
      "Epoch 26/50\n",
      "509/509 [==============================] - 2s 3ms/step - loss: 0.3593 - acc: 0.8782\n",
      "Epoch 27/50\n",
      "509/509 [==============================] - 2s 3ms/step - loss: 0.2908 - acc: 0.8900\n",
      "Epoch 28/50\n",
      "509/509 [==============================] - 2s 3ms/step - loss: 0.2041 - acc: 0.9293\n",
      "Epoch 29/50\n",
      "509/509 [==============================] - 2s 3ms/step - loss: 0.1729 - acc: 0.9470\n",
      "Epoch 30/50\n",
      "509/509 [==============================] - 2s 3ms/step - loss: 0.1468 - acc: 0.9528\n",
      "Epoch 31/50\n",
      "509/509 [==============================] - 2s 3ms/step - loss: 0.1332 - acc: 0.9627\n",
      "Epoch 32/50\n",
      "509/509 [==============================] - 2s 3ms/step - loss: 0.1288 - acc: 0.9646\n",
      "Epoch 33/50\n",
      "509/509 [==============================] - 2s 3ms/step - loss: 0.0994 - acc: 0.9745\n",
      "Epoch 34/50\n",
      "509/509 [==============================] - 2s 3ms/step - loss: 0.1277 - acc: 0.9548\n",
      "Epoch 35/50\n",
      "509/509 [==============================] - 2s 3ms/step - loss: 0.1729 - acc: 0.9450\n",
      "Epoch 36/50\n",
      "509/509 [==============================] - 2s 3ms/step - loss: 0.1139 - acc: 0.9705\n",
      "Epoch 37/50\n",
      "509/509 [==============================] - 2s 3ms/step - loss: 0.1286 - acc: 0.9705\n",
      "Epoch 38/50\n",
      "509/509 [==============================] - 2s 3ms/step - loss: 0.0750 - acc: 0.9804\n",
      "Epoch 39/50\n",
      "509/509 [==============================] - 2s 3ms/step - loss: 0.0719 - acc: 0.9784\n",
      "Epoch 40/50\n",
      "509/509 [==============================] - 2s 3ms/step - loss: 0.0600 - acc: 0.9862\n",
      "Epoch 41/50\n",
      "509/509 [==============================] - 2s 3ms/step - loss: 0.0560 - acc: 0.9882\n",
      "Epoch 42/50\n",
      "509/509 [==============================] - 2s 3ms/step - loss: 0.0578 - acc: 0.9902\n",
      "Epoch 43/50\n",
      "509/509 [==============================] - 2s 3ms/step - loss: 0.0522 - acc: 0.9921\n",
      "Epoch 44/50\n",
      "509/509 [==============================] - 2s 3ms/step - loss: 0.0414 - acc: 0.9921\n",
      "Epoch 45/50\n",
      "509/509 [==============================] - 2s 3ms/step - loss: 0.0384 - acc: 0.9941\n",
      "Epoch 46/50\n",
      "509/509 [==============================] - 2s 3ms/step - loss: 0.0445 - acc: 0.9921\n",
      "Epoch 47/50\n",
      "509/509 [==============================] - 2s 3ms/step - loss: 0.0448 - acc: 0.9921\n",
      "Epoch 48/50\n",
      "509/509 [==============================] - 2s 3ms/step - loss: 0.0333 - acc: 0.9941\n",
      "Epoch 49/50\n",
      "509/509 [==============================] - 2s 3ms/step - loss: 0.0322 - acc: 0.9941\n",
      "Epoch 50/50\n",
      "509/509 [==============================] - 2s 3ms/step - loss: 0.0759 - acc: 0.9823\n",
      "\n",
      "Training LSTM on fold 3 / 5 :\n",
      "\n",
      "Epoch 1/50\n",
      "509/509 [==============================] - 4s 9ms/step - loss: 0.5787 - acc: 0.7289\n",
      "Epoch 2/50\n",
      "509/509 [==============================] - 1s 3ms/step - loss: 0.5587 - acc: 0.7603\n",
      "Epoch 3/50\n",
      "509/509 [==============================] - 1s 3ms/step - loss: 0.5636 - acc: 0.7603\n",
      "Epoch 4/50\n",
      "509/509 [==============================] - 1s 3ms/step - loss: 0.5723 - acc: 0.7603\n",
      "Epoch 5/50\n",
      "509/509 [==============================] - 1s 3ms/step - loss: 0.5547 - acc: 0.7623\n",
      "Epoch 6/50\n",
      "509/509 [==============================] - 2s 3ms/step - loss: 0.5514 - acc: 0.7642\n",
      "Epoch 7/50\n",
      "509/509 [==============================] - 2s 3ms/step - loss: 0.5573 - acc: 0.7642\n",
      "Epoch 8/50\n",
      "509/509 [==============================] - 2s 3ms/step - loss: 0.5506 - acc: 0.7642\n",
      "Epoch 9/50\n",
      "509/509 [==============================] - 2s 3ms/step - loss: 0.5564 - acc: 0.7642\n",
      "Epoch 10/50\n",
      "509/509 [==============================] - 2s 3ms/step - loss: 0.5240 - acc: 0.7662\n",
      "Epoch 11/50\n",
      "509/509 [==============================] - 2s 3ms/step - loss: 0.4307 - acc: 0.8448\n",
      "Epoch 12/50\n",
      "509/509 [==============================] - 2s 3ms/step - loss: 0.5198 - acc: 0.7937\n",
      "Epoch 13/50\n",
      "509/509 [==============================] - 2s 3ms/step - loss: 0.5248 - acc: 0.8350\n",
      "Epoch 14/50\n",
      "509/509 [==============================] - 2s 3ms/step - loss: 0.6222 - acc: 0.6837\n",
      "Epoch 15/50\n",
      "509/509 [==============================] - 2s 3ms/step - loss: 0.5411 - acc: 0.7387\n",
      "Epoch 16/50\n",
      "509/509 [==============================] - 2s 3ms/step - loss: 0.5309 - acc: 0.7525\n",
      "Epoch 17/50\n",
      "509/509 [==============================] - 2s 3ms/step - loss: 0.5406 - acc: 0.7367\n",
      "Epoch 18/50\n",
      "509/509 [==============================] - 2s 3ms/step - loss: 0.5404 - acc: 0.7367\n",
      "Epoch 19/50\n",
      "509/509 [==============================] - 2s 3ms/step - loss: 0.5422 - acc: 0.7564\n",
      "Epoch 20/50\n",
      "509/509 [==============================] - 2s 3ms/step - loss: 0.5208 - acc: 0.7446\n",
      "Epoch 21/50\n",
      "509/509 [==============================] - 2s 3ms/step - loss: 0.4515 - acc: 0.7878\n",
      "Epoch 22/50\n",
      "509/509 [==============================] - 2s 3ms/step - loss: 0.4430 - acc: 0.7839\n",
      "Epoch 23/50\n",
      "509/509 [==============================] - 2s 3ms/step - loss: 0.4478 - acc: 0.7878\n",
      "Epoch 24/50\n",
      "509/509 [==============================] - 2s 3ms/step - loss: 0.4379 - acc: 0.7878\n",
      "Epoch 25/50\n",
      "509/509 [==============================] - 2s 3ms/step - loss: 0.4250 - acc: 0.8310\n",
      "Epoch 26/50\n",
      "509/509 [==============================] - 2s 3ms/step - loss: 0.4319 - acc: 0.8310\n",
      "Epoch 27/50\n",
      "509/509 [==============================] - 2s 3ms/step - loss: 0.4313 - acc: 0.8271\n",
      "Epoch 28/50\n",
      "509/509 [==============================] - 2s 3ms/step - loss: 0.4290 - acc: 0.8173\n",
      "Epoch 29/50\n",
      "509/509 [==============================] - 2s 3ms/step - loss: 0.4273 - acc: 0.8212\n",
      "Epoch 30/50\n",
      "509/509 [==============================] - 2s 3ms/step - loss: 0.4325 - acc: 0.8291\n",
      "Epoch 31/50\n",
      "509/509 [==============================] - 2s 3ms/step - loss: 0.4270 - acc: 0.8271\n",
      "Epoch 32/50\n",
      "509/509 [==============================] - 2s 3ms/step - loss: 0.4311 - acc: 0.8212\n",
      "Epoch 33/50\n",
      "509/509 [==============================] - 2s 3ms/step - loss: 0.4369 - acc: 0.8310\n",
      "Epoch 34/50\n",
      "509/509 [==============================] - 2s 3ms/step - loss: 0.4252 - acc: 0.8291\n",
      "Epoch 35/50\n",
      "509/509 [==============================] - 2s 3ms/step - loss: 0.4271 - acc: 0.8271\n",
      "Epoch 36/50\n",
      "509/509 [==============================] - 2s 3ms/step - loss: 0.4326 - acc: 0.8232\n",
      "Epoch 37/50\n",
      "509/509 [==============================] - 2s 3ms/step - loss: 0.4330 - acc: 0.8173\n",
      "Epoch 38/50\n",
      "509/509 [==============================] - 2s 3ms/step - loss: 0.4303 - acc: 0.8409\n",
      "Epoch 39/50\n",
      "509/509 [==============================] - 2s 3ms/step - loss: 0.4236 - acc: 0.8310\n",
      "Epoch 40/50\n",
      "509/509 [==============================] - 2s 3ms/step - loss: 0.4276 - acc: 0.8291\n",
      "Epoch 41/50\n",
      "509/509 [==============================] - 2s 3ms/step - loss: 0.4250 - acc: 0.8232\n",
      "Epoch 42/50\n",
      "509/509 [==============================] - 2s 3ms/step - loss: 0.4368 - acc: 0.7721\n",
      "Epoch 43/50\n",
      "509/509 [==============================] - 2s 3ms/step - loss: 0.5245 - acc: 0.7033\n",
      "Epoch 44/50\n",
      "509/509 [==============================] - 2s 3ms/step - loss: 0.4609 - acc: 0.7741\n",
      "Epoch 45/50\n",
      "509/509 [==============================] - 2s 3ms/step - loss: 0.4461 - acc: 0.7819\n",
      "Epoch 46/50\n",
      "509/509 [==============================] - 2s 3ms/step - loss: 0.4481 - acc: 0.7328\n",
      "Epoch 47/50\n",
      "509/509 [==============================] - 2s 3ms/step - loss: 0.4441 - acc: 0.7112\n",
      "Epoch 48/50\n",
      "509/509 [==============================] - 2s 3ms/step - loss: 0.4426 - acc: 0.7230\n",
      "Epoch 49/50\n",
      "509/509 [==============================] - 2s 3ms/step - loss: 0.4381 - acc: 0.7642\n",
      "Epoch 50/50\n",
      "509/509 [==============================] - 2s 3ms/step - loss: 0.4461 - acc: 0.7544\n",
      "\n",
      "Training LSTM on fold 4 / 5 :\n",
      "\n",
      "Epoch 1/50\n",
      "510/510 [==============================] - 5s 9ms/step - loss: 0.5640 - acc: 0.7588\n",
      "Epoch 2/50\n",
      "510/510 [==============================] - 2s 3ms/step - loss: 0.5574 - acc: 0.7588\n",
      "Epoch 3/50\n",
      "510/510 [==============================] - 1s 3ms/step - loss: 0.5661 - acc: 0.7627\n",
      "Epoch 4/50\n",
      "510/510 [==============================] - 1s 3ms/step - loss: 0.5756 - acc: 0.7627\n",
      "Epoch 5/50\n",
      "510/510 [==============================] - 2s 3ms/step - loss: 0.5743 - acc: 0.7627\n",
      "Epoch 6/50\n",
      "510/510 [==============================] - 2s 3ms/step - loss: 0.5572 - acc: 0.7627\n",
      "Epoch 7/50\n",
      "510/510 [==============================] - 2s 3ms/step - loss: 0.5458 - acc: 0.7647\n",
      "Epoch 8/50\n",
      "510/510 [==============================] - 2s 3ms/step - loss: 0.5434 - acc: 0.7804\n",
      "Epoch 9/50\n",
      "510/510 [==============================] - 2s 3ms/step - loss: 0.5586 - acc: 0.7608\n",
      "Epoch 10/50\n",
      "510/510 [==============================] - 2s 4ms/step - loss: 0.5547 - acc: 0.7627\n",
      "Epoch 11/50\n",
      "510/510 [==============================] - 2s 3ms/step - loss: 0.5538 - acc: 0.7627\n",
      "Epoch 12/50\n",
      "510/510 [==============================] - 2s 3ms/step - loss: 0.4365 - acc: 0.8431\n",
      "Epoch 13/50\n",
      "510/510 [==============================] - 2s 3ms/step - loss: 0.4089 - acc: 0.8765\n",
      "Epoch 14/50\n",
      "510/510 [==============================] - 2s 3ms/step - loss: 0.4397 - acc: 0.8451\n",
      "Epoch 15/50\n",
      "510/510 [==============================] - 2s 3ms/step - loss: 0.4333 - acc: 0.8412\n",
      "Epoch 16/50\n",
      "510/510 [==============================] - 2s 3ms/step - loss: 0.3708 - acc: 0.8647\n",
      "Epoch 17/50\n",
      "510/510 [==============================] - 2s 3ms/step - loss: 0.3395 - acc: 0.8961A: 1s - loss: 0.3796 - \n",
      "Epoch 18/50\n",
      "510/510 [==============================] - 2s 3ms/step - loss: 0.2610 - acc: 0.9196\n",
      "Epoch 19/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "510/510 [==============================] - 2s 3ms/step - loss: 0.2416 - acc: 0.9235\n",
      "Epoch 20/50\n",
      "510/510 [==============================] - 2s 3ms/step - loss: 0.1736 - acc: 0.9471\n",
      "Epoch 21/50\n",
      "510/510 [==============================] - 2s 3ms/step - loss: 0.1275 - acc: 0.9627\n",
      "Epoch 22/50\n",
      "510/510 [==============================] - 2s 3ms/step - loss: 0.1235 - acc: 0.9686\n",
      "Epoch 23/50\n",
      "510/510 [==============================] - 2s 3ms/step - loss: 0.1782 - acc: 0.9471\n",
      "Epoch 24/50\n",
      "510/510 [==============================] - 2s 3ms/step - loss: 0.0892 - acc: 0.9784\n",
      "Epoch 25/50\n",
      "510/510 [==============================] - 2s 3ms/step - loss: 0.0735 - acc: 0.9804\n",
      "Epoch 26/50\n",
      "510/510 [==============================] - 2s 3ms/step - loss: 0.0460 - acc: 0.9902\n",
      "Epoch 27/50\n",
      "510/510 [==============================] - 2s 3ms/step - loss: 0.1028 - acc: 0.9627\n",
      "Epoch 28/50\n",
      "510/510 [==============================] - 2s 3ms/step - loss: 0.0316 - acc: 0.9902\n",
      "Epoch 29/50\n",
      "510/510 [==============================] - 2s 3ms/step - loss: 0.0276 - acc: 0.9961\n",
      "Epoch 30/50\n",
      "510/510 [==============================] - 2s 3ms/step - loss: 0.4435 - acc: 0.8784\n",
      "Epoch 31/50\n",
      "510/510 [==============================] - 2s 3ms/step - loss: 0.3355 - acc: 0.8941\n",
      "Epoch 32/50\n",
      "510/510 [==============================] - 2s 3ms/step - loss: 0.1790 - acc: 0.9412\n",
      "Epoch 33/50\n",
      "510/510 [==============================] - 2s 3ms/step - loss: 0.1114 - acc: 0.9608\n",
      "Epoch 34/50\n",
      "510/510 [==============================] - 2s 3ms/step - loss: 0.0651 - acc: 0.9804\n",
      "Epoch 35/50\n",
      "510/510 [==============================] - 2s 3ms/step - loss: 0.0279 - acc: 0.9980\n",
      "Epoch 36/50\n",
      "510/510 [==============================] - 2s 3ms/step - loss: 0.0165 - acc: 0.9980\n",
      "Epoch 37/50\n",
      "510/510 [==============================] - 2s 3ms/step - loss: 0.0130 - acc: 0.9980\n",
      "Epoch 38/50\n",
      "510/510 [==============================] - 2s 3ms/step - loss: 0.0173 - acc: 0.9980\n",
      "Epoch 39/50\n",
      "510/510 [==============================] - 2s 3ms/step - loss: 0.0144 - acc: 0.9980\n",
      "Epoch 40/50\n",
      "510/510 [==============================] - 2s 3ms/step - loss: 0.0137 - acc: 0.9980\n",
      "Epoch 41/50\n",
      "510/510 [==============================] - 2s 3ms/step - loss: 0.0129 - acc: 0.9980\n",
      "Epoch 42/50\n",
      "510/510 [==============================] - 2s 3ms/step - loss: 0.0143 - acc: 0.9980\n",
      "Epoch 43/50\n",
      "510/510 [==============================] - 2s 3ms/step - loss: 0.0143 - acc: 0.9980\n",
      "Epoch 44/50\n",
      "510/510 [==============================] - 2s 3ms/step - loss: 0.0147 - acc: 0.9980\n",
      "Epoch 45/50\n",
      "510/510 [==============================] - 2s 3ms/step - loss: 0.0143 - acc: 0.9980\n",
      "Epoch 46/50\n",
      "510/510 [==============================] - 2s 3ms/step - loss: 0.0155 - acc: 0.9980\n",
      "Epoch 47/50\n",
      "510/510 [==============================] - 2s 4ms/step - loss: 0.0145 - acc: 0.9980\n",
      "Epoch 48/50\n",
      "510/510 [==============================] - 2s 3ms/step - loss: 0.0134 - acc: 0.9980\n",
      "Epoch 49/50\n",
      "510/510 [==============================] - 2s 3ms/step - loss: 0.0155 - acc: 0.9980\n",
      "Epoch 50/50\n",
      "510/510 [==============================] - 2s 3ms/step - loss: 0.0163 - acc: 0.9980\n",
      "\n",
      "Training LSTM on fold 5 / 5 :\n",
      "\n",
      "Epoch 1/50\n",
      "511/511 [==============================] - 5s 9ms/step - loss: 0.5992 - acc: 0.7319\n",
      "Epoch 2/50\n",
      "511/511 [==============================] - 1s 3ms/step - loss: 0.5627 - acc: 0.7593\n",
      "Epoch 3/50\n",
      "511/511 [==============================] - 1s 3ms/step - loss: 0.5632 - acc: 0.7593\n",
      "Epoch 4/50\n",
      "511/511 [==============================] - 1s 3ms/step - loss: 0.5526 - acc: 0.7593\n",
      "Epoch 5/50\n",
      "511/511 [==============================] - 2s 3ms/step - loss: 0.5656 - acc: 0.7593\n",
      "Epoch 6/50\n",
      "511/511 [==============================] - 2s 3ms/step - loss: 0.5515 - acc: 0.7593\n",
      "Epoch 7/50\n",
      "511/511 [==============================] - 2s 3ms/step - loss: 0.5608 - acc: 0.7613\n",
      "Epoch 8/50\n",
      "511/511 [==============================] - 2s 3ms/step - loss: 0.5661 - acc: 0.7613\n",
      "Epoch 9/50\n",
      "511/511 [==============================] - 2s 3ms/step - loss: 0.5568 - acc: 0.7613\n",
      "Epoch 10/50\n",
      "511/511 [==============================] - 2s 3ms/step - loss: 0.5561 - acc: 0.7613\n",
      "Epoch 11/50\n",
      "511/511 [==============================] - 2s 3ms/step - loss: 0.5665 - acc: 0.7613\n",
      "Epoch 12/50\n",
      "511/511 [==============================] - 2s 3ms/step - loss: 0.5513 - acc: 0.7613\n",
      "Epoch 13/50\n",
      "511/511 [==============================] - 2s 3ms/step - loss: 0.5542 - acc: 0.7613\n",
      "Epoch 14/50\n",
      "511/511 [==============================] - 2s 3ms/step - loss: 0.5545 - acc: 0.7632\n",
      "Epoch 15/50\n",
      "511/511 [==============================] - 2s 3ms/step - loss: 0.3928 - acc: 0.8513\n",
      "Epoch 16/50\n",
      "511/511 [==============================] - 2s 3ms/step - loss: 0.5545 - acc: 0.7554\n",
      "Epoch 17/50\n",
      "511/511 [==============================] - 2s 3ms/step - loss: 0.4293 - acc: 0.7945\n",
      "Epoch 18/50\n",
      "511/511 [==============================] - 2s 3ms/step - loss: 0.3406 - acc: 0.8924\n",
      "Epoch 19/50\n",
      "511/511 [==============================] - 2s 3ms/step - loss: 0.2844 - acc: 0.9139\n",
      "Epoch 20/50\n",
      "511/511 [==============================] - 2s 3ms/step - loss: 0.2428 - acc: 0.9198\n",
      "Epoch 21/50\n",
      "511/511 [==============================] - 2s 3ms/step - loss: 0.1969 - acc: 0.9393\n",
      "Epoch 22/50\n",
      "511/511 [==============================] - 2s 3ms/step - loss: 0.1699 - acc: 0.9511\n",
      "Epoch 23/50\n",
      "511/511 [==============================] - 2s 3ms/step - loss: 0.1275 - acc: 0.9648\n",
      "Epoch 24/50\n",
      "511/511 [==============================] - 2s 3ms/step - loss: 0.1279 - acc: 0.9648\n",
      "Epoch 25/50\n",
      "511/511 [==============================] - 2s 3ms/step - loss: 0.1068 - acc: 0.9628\n",
      "Epoch 26/50\n",
      "511/511 [==============================] - 2s 3ms/step - loss: 0.1031 - acc: 0.9726\n",
      "Epoch 27/50\n",
      "511/511 [==============================] - 2s 3ms/step - loss: 0.0885 - acc: 0.9804\n",
      "Epoch 28/50\n",
      "511/511 [==============================] - 2s 3ms/step - loss: 0.0777 - acc: 0.9843\n",
      "Epoch 29/50\n",
      "511/511 [==============================] - 2s 3ms/step - loss: 0.0642 - acc: 0.9843\n",
      "Epoch 30/50\n",
      "511/511 [==============================] - 2s 3ms/step - loss: 0.0576 - acc: 0.9863\n",
      "Epoch 31/50\n",
      "511/511 [==============================] - 2s 3ms/step - loss: 0.0642 - acc: 0.9843\n",
      "Epoch 32/50\n",
      "511/511 [==============================] - 2s 3ms/step - loss: 0.0972 - acc: 0.9785\n",
      "Epoch 33/50\n",
      "511/511 [==============================] - 2s 3ms/step - loss: 0.2061 - acc: 0.9374\n",
      "Epoch 34/50\n",
      "511/511 [==============================] - 2s 3ms/step - loss: 0.1273 - acc: 0.9550\n",
      "Epoch 35/50\n",
      "511/511 [==============================] - 2s 3ms/step - loss: 0.0740 - acc: 0.9785\n",
      "Epoch 36/50\n",
      "511/511 [==============================] - 2s 3ms/step - loss: 0.0531 - acc: 0.9883\n",
      "Epoch 37/50\n",
      "511/511 [==============================] - 2s 3ms/step - loss: 0.0499 - acc: 0.9843\n",
      "Epoch 38/50\n",
      "511/511 [==============================] - 2s 3ms/step - loss: 0.0340 - acc: 0.9922\n",
      "Epoch 39/50\n",
      "511/511 [==============================] - 2s 3ms/step - loss: 0.0394 - acc: 0.9883\n",
      "Epoch 40/50\n",
      "511/511 [==============================] - 2s 3ms/step - loss: 0.0449 - acc: 0.9902\n",
      "Epoch 41/50\n",
      "511/511 [==============================] - 2s 3ms/step - loss: 0.0278 - acc: 0.9941\n",
      "Epoch 42/50\n",
      "511/511 [==============================] - 2s 3ms/step - loss: 0.0257 - acc: 0.9922\n",
      "Epoch 43/50\n",
      "511/511 [==============================] - 2s 3ms/step - loss: 0.0233 - acc: 0.9941\n",
      "Epoch 44/50\n",
      "511/511 [==============================] - 2s 3ms/step - loss: 0.0228 - acc: 0.9941\n",
      "Epoch 45/50\n",
      "511/511 [==============================] - 2s 3ms/step - loss: 0.0165 - acc: 0.9961\n",
      "Epoch 46/50\n",
      "511/511 [==============================] - 2s 3ms/step - loss: 0.0171 - acc: 0.9961\n",
      "Epoch 47/50\n",
      "511/511 [==============================] - 2s 3ms/step - loss: 0.0166 - acc: 0.9980\n",
      "Epoch 48/50\n",
      "511/511 [==============================] - 2s 3ms/step - loss: 0.0074 - acc: 0.9980\n",
      "Epoch 49/50\n",
      "511/511 [==============================] - 2s 3ms/step - loss: 0.0034 - acc: 0.9980\n",
      "Epoch 50/50\n",
      "511/511 [==============================] - 2s 3ms/step - loss: 0.0017 - acc: 1.0000\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "n_folds = 5\n",
    "n_epochs = 10**3\n",
    "\n",
    "skf = StratifiedKFold(n_splits = n_folds, random_state = 33, shuffle=True)\n",
    "\n",
    "out_of_fold_preds = np.nan*np.ones([embedding_matrix.shape[0], 1])\n",
    "#predictions made when the given data point was in holdout set\n",
    "\n",
    "\n",
    "#loop over k-fold splits\n",
    "for k, (train_indices, test_indices) in enumerate(skf.split(embedding_matrix, labels)):\n",
    "    print('\\nTraining LSTM on fold %d / %d :\\n'%(k+1, n_folds))\n",
    "    model = LSTM_graph(max_len, glove_df)\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    model.fit(embedding_matrix[train_indices,:], labels[train_indices], epochs=50, batch_size=32, shuffle=True)\n",
    "    out_of_fold_preds[test_indices,:] = model.predict(embedding_matrix[test_indices,:])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[9.96589959e-01],\n",
       "       [9.96329784e-01],\n",
       "       [9.99440074e-01],\n",
       "       [9.39141750e-01],\n",
       "       [9.98056769e-01],\n",
       "       [9.97541726e-01],\n",
       "       [9.97922659e-01],\n",
       "       [9.99444366e-01],\n",
       "       [9.38325882e-01],\n",
       "       [9.99361634e-01],\n",
       "       [9.99371171e-01],\n",
       "       [9.39458609e-01],\n",
       "       [9.37644124e-01],\n",
       "       [9.99439836e-01],\n",
       "       [9.98959661e-01],\n",
       "       [9.98264968e-01],\n",
       "       [9.99373078e-01],\n",
       "       [9.37231779e-01],\n",
       "       [9.98103321e-01],\n",
       "       [9.96634066e-01],\n",
       "       [9.98261452e-01],\n",
       "       [9.97969866e-01],\n",
       "       [9.99384165e-01],\n",
       "       [9.98260617e-01],\n",
       "       [9.99387622e-01],\n",
       "       [9.99435902e-01],\n",
       "       [9.96303916e-01],\n",
       "       [9.98099089e-01],\n",
       "       [9.99459028e-01],\n",
       "       [3.27229500e-04],\n",
       "       [9.95953977e-01],\n",
       "       [9.97938991e-01],\n",
       "       [8.52109790e-01],\n",
       "       [9.99406695e-01],\n",
       "       [9.39127803e-01],\n",
       "       [9.98246372e-01],\n",
       "       [9.99444604e-01],\n",
       "       [9.98222828e-01],\n",
       "       [9.96633351e-01],\n",
       "       [9.97218966e-01],\n",
       "       [9.98190939e-01],\n",
       "       [9.98268068e-01],\n",
       "       [9.96445179e-01],\n",
       "       [9.99426365e-01],\n",
       "       [9.98109102e-01],\n",
       "       [9.96608377e-01],\n",
       "       [9.96446848e-01],\n",
       "       [9.98258948e-01],\n",
       "       [9.97964740e-01],\n",
       "       [9.94756520e-01],\n",
       "       [9.96215403e-01],\n",
       "       [9.38690424e-01],\n",
       "       [9.98233974e-01],\n",
       "       [9.98292387e-01],\n",
       "       [9.97933865e-01],\n",
       "       [9.98339415e-01],\n",
       "       [9.98101711e-01],\n",
       "       [9.96618867e-01],\n",
       "       [9.37735498e-01],\n",
       "       [9.37113285e-01],\n",
       "       [9.98260200e-01],\n",
       "       [5.49901307e-01],\n",
       "       [9.97980416e-01],\n",
       "       [9.97822702e-01],\n",
       "       [9.98235703e-01],\n",
       "       [9.98264432e-01],\n",
       "       [9.98071313e-01],\n",
       "       [9.96452212e-01],\n",
       "       [9.98272419e-01],\n",
       "       [5.49901366e-01],\n",
       "       [5.49901366e-01],\n",
       "       [9.99222219e-01],\n",
       "       [9.96487260e-01],\n",
       "       [9.36707914e-01],\n",
       "       [9.98225033e-01],\n",
       "       [9.37809229e-01],\n",
       "       [9.98247743e-01],\n",
       "       [9.99463558e-01],\n",
       "       [7.92481661e-01],\n",
       "       [9.37394142e-01],\n",
       "       [9.99449253e-01],\n",
       "       [9.98273134e-01],\n",
       "       [9.99431610e-01],\n",
       "       [9.96414602e-01],\n",
       "       [9.38202202e-01],\n",
       "       [9.98250484e-01],\n",
       "       [9.96632218e-01],\n",
       "       [9.96609330e-01],\n",
       "       [3.57048512e-01],\n",
       "       [9.98032451e-01],\n",
       "       [9.98218536e-01],\n",
       "       [9.99471784e-01],\n",
       "       [6.59283638e-01],\n",
       "       [9.96568561e-01],\n",
       "       [9.99443889e-01],\n",
       "       [9.38896060e-01],\n",
       "       [9.87784564e-01],\n",
       "       [9.98207808e-01],\n",
       "       [9.96420681e-01],\n",
       "       [9.90360677e-01],\n",
       "       [9.37934160e-01],\n",
       "       [9.40842807e-01],\n",
       "       [9.98156667e-01],\n",
       "       [2.63947248e-03],\n",
       "       [9.99362946e-01],\n",
       "       [9.39479947e-01],\n",
       "       [9.97578263e-01],\n",
       "       [9.96610880e-01],\n",
       "       [9.99213040e-01],\n",
       "       [9.99420583e-01],\n",
       "       [9.96362686e-01],\n",
       "       [8.38947892e-01],\n",
       "       [9.97912109e-01],\n",
       "       [9.98172998e-01],\n",
       "       [9.96609092e-01],\n",
       "       [9.99445081e-01],\n",
       "       [9.99397755e-01],\n",
       "       [9.98205543e-01],\n",
       "       [9.99428868e-01],\n",
       "       [9.37464237e-01],\n",
       "       [9.98255491e-01],\n",
       "       [9.98155296e-01],\n",
       "       [9.98048306e-01],\n",
       "       [9.98255253e-01],\n",
       "       [9.97321784e-01],\n",
       "       [9.05222416e-01],\n",
       "       [9.98199463e-01],\n",
       "       [9.99413073e-01],\n",
       "       [9.99462545e-01],\n",
       "       [9.39730525e-01],\n",
       "       [9.99435306e-01],\n",
       "       [9.96512413e-01],\n",
       "       [9.95138824e-01],\n",
       "       [5.49901783e-01],\n",
       "       [9.98232126e-01],\n",
       "       [5.49901247e-01],\n",
       "       [9.98236179e-01],\n",
       "       [9.98219609e-01],\n",
       "       [9.68860388e-01],\n",
       "       [9.81394291e-01],\n",
       "       [9.38019574e-01],\n",
       "       [9.96552944e-01],\n",
       "       [9.98290777e-01],\n",
       "       [9.98043776e-01],\n",
       "       [9.98099625e-01],\n",
       "       [9.36788201e-01],\n",
       "       [9.97244596e-01],\n",
       "       [7.26243258e-01],\n",
       "       [9.97612178e-01],\n",
       "       [9.97688353e-01],\n",
       "       [9.98128533e-01],\n",
       "       [9.97766912e-01],\n",
       "       [9.38712716e-01],\n",
       "       [9.39019084e-01],\n",
       "       [5.49901545e-01],\n",
       "       [9.96617198e-01],\n",
       "       [9.98115003e-01],\n",
       "       [9.40433741e-01],\n",
       "       [9.98224795e-01],\n",
       "       [9.98302758e-01],\n",
       "       [9.40634310e-01],\n",
       "       [9.99408960e-01],\n",
       "       [7.71373510e-04],\n",
       "       [9.97524500e-01],\n",
       "       [9.99391377e-01],\n",
       "       [9.97368872e-01],\n",
       "       [9.98084545e-01],\n",
       "       [9.98005033e-01],\n",
       "       [9.97167468e-01],\n",
       "       [8.35784435e-01],\n",
       "       [9.97644424e-01],\n",
       "       [9.96123195e-01],\n",
       "       [5.49901247e-01],\n",
       "       [9.98260736e-01],\n",
       "       [9.98118997e-01],\n",
       "       [9.96002495e-01],\n",
       "       [9.98262882e-01],\n",
       "       [9.96251106e-01],\n",
       "       [9.97824788e-01],\n",
       "       [9.96594667e-01],\n",
       "       [9.97869611e-01],\n",
       "       [9.96036768e-01],\n",
       "       [9.98229861e-01],\n",
       "       [9.98214841e-01],\n",
       "       [9.96177912e-01],\n",
       "       [9.99443650e-01],\n",
       "       [9.98224378e-01],\n",
       "       [9.98362541e-01],\n",
       "       [9.96583462e-01],\n",
       "       [9.96082306e-01],\n",
       "       [9.98296320e-01],\n",
       "       [9.98166800e-01],\n",
       "       [9.96444225e-01],\n",
       "       [9.98221755e-01],\n",
       "       [9.99434233e-01],\n",
       "       [9.99434233e-01],\n",
       "       [5.49901187e-01],\n",
       "       [9.96536493e-01],\n",
       "       [9.98219252e-01],\n",
       "       [9.41130102e-01],\n",
       "       [9.96225119e-01],\n",
       "       [9.99368668e-01],\n",
       "       [9.98211384e-01],\n",
       "       [9.98219132e-01],\n",
       "       [9.40502167e-01],\n",
       "       [9.97937739e-01],\n",
       "       [9.39561903e-01],\n",
       "       [9.38445747e-01],\n",
       "       [8.28130841e-01],\n",
       "       [9.38887715e-01],\n",
       "       [9.99387622e-01],\n",
       "       [9.98061776e-01],\n",
       "       [9.98177767e-01],\n",
       "       [9.98176098e-01],\n",
       "       [9.99455512e-01],\n",
       "       [9.98257875e-01],\n",
       "       [9.99424160e-01],\n",
       "       [9.99397159e-01],\n",
       "       [9.95422721e-01],\n",
       "       [9.40880656e-01],\n",
       "       [9.40202236e-01],\n",
       "       [9.98168468e-01],\n",
       "       [9.99416590e-01],\n",
       "       [9.97880936e-01],\n",
       "       [9.40446734e-01],\n",
       "       [9.99262214e-01],\n",
       "       [9.99337077e-01],\n",
       "       [9.98028517e-01],\n",
       "       [9.98080134e-01],\n",
       "       [9.96422172e-01],\n",
       "       [9.41927016e-01],\n",
       "       [9.96387601e-01],\n",
       "       [9.96172845e-01],\n",
       "       [9.98273134e-01],\n",
       "       [9.98046398e-01],\n",
       "       [9.98228490e-01],\n",
       "       [9.96502280e-01],\n",
       "       [9.96248484e-01],\n",
       "       [9.98268127e-01],\n",
       "       [9.98201013e-01],\n",
       "       [9.37425375e-01],\n",
       "       [9.40237284e-01],\n",
       "       [9.98253763e-01],\n",
       "       [9.99432623e-01],\n",
       "       [9.98164535e-01],\n",
       "       [9.96199727e-01],\n",
       "       [9.99439538e-01],\n",
       "       [9.97801185e-01],\n",
       "       [9.99339938e-01],\n",
       "       [9.96004522e-01],\n",
       "       [5.49901187e-01],\n",
       "       [5.49901187e-01],\n",
       "       [9.97802854e-01],\n",
       "       [9.96526122e-01],\n",
       "       [9.38353300e-01],\n",
       "       [9.38003659e-01],\n",
       "       [9.94487524e-01],\n",
       "       [9.90437627e-01],\n",
       "       [9.98229861e-01],\n",
       "       [9.96625304e-01],\n",
       "       [9.96364057e-01],\n",
       "       [9.96624112e-01],\n",
       "       [9.40914273e-01],\n",
       "       [9.97332215e-01],\n",
       "       [9.97030616e-01],\n",
       "       [9.98237729e-01],\n",
       "       [9.96599436e-01],\n",
       "       [9.97499466e-01],\n",
       "       [9.99379098e-01],\n",
       "       [9.98272598e-01],\n",
       "       [9.96572554e-01],\n",
       "       [9.40348148e-01],\n",
       "       [9.98263121e-01],\n",
       "       [9.97877061e-01],\n",
       "       [9.41797137e-01],\n",
       "       [9.98299360e-01],\n",
       "       [9.95972037e-01],\n",
       "       [9.99342501e-01],\n",
       "       [9.99464273e-01],\n",
       "       [9.96293783e-01],\n",
       "       [9.96554494e-01],\n",
       "       [9.98261094e-01],\n",
       "       [9.97844279e-01],\n",
       "       [9.99455333e-01],\n",
       "       [9.97646213e-01],\n",
       "       [9.98024583e-01],\n",
       "       [9.97602105e-01],\n",
       "       [9.97807503e-01],\n",
       "       [9.95449901e-01],\n",
       "       [9.96519029e-01],\n",
       "       [9.96459365e-01],\n",
       "       [9.37938571e-01],\n",
       "       [9.98146653e-01],\n",
       "       [9.98254299e-01],\n",
       "       [9.96388674e-01],\n",
       "       [9.98139024e-01],\n",
       "       [9.98184443e-01],\n",
       "       [9.96421814e-01],\n",
       "       [7.82147050e-03],\n",
       "       [9.97215748e-01],\n",
       "       [9.99444962e-01],\n",
       "       [9.96549010e-01],\n",
       "       [9.97894704e-01],\n",
       "       [9.39186096e-01],\n",
       "       [9.39944565e-01],\n",
       "       [9.98214900e-01],\n",
       "       [9.96479154e-01],\n",
       "       [5.49901187e-01],\n",
       "       [9.98063743e-01],\n",
       "       [9.39931512e-01],\n",
       "       [9.96450901e-01],\n",
       "       [9.99388337e-01],\n",
       "       [9.98037457e-01],\n",
       "       [9.97310638e-01],\n",
       "       [9.98224914e-01],\n",
       "       [9.98270750e-01],\n",
       "       [9.98010933e-01],\n",
       "       [9.96637821e-01],\n",
       "       [9.98039484e-01],\n",
       "       [9.99421597e-01],\n",
       "       [7.86817074e-03],\n",
       "       [9.98250842e-01],\n",
       "       [9.98032212e-01],\n",
       "       [9.97912407e-01],\n",
       "       [9.99336541e-01],\n",
       "       [9.96526659e-01],\n",
       "       [9.39163327e-01],\n",
       "       [9.38702822e-01],\n",
       "       [9.37781811e-01],\n",
       "       [9.98210669e-01],\n",
       "       [9.98097062e-01],\n",
       "       [7.08222389e-04],\n",
       "       [9.99364495e-01],\n",
       "       [9.99390483e-01],\n",
       "       [3.90708447e-04],\n",
       "       [9.92007852e-01],\n",
       "       [9.96623158e-01],\n",
       "       [9.97496307e-01],\n",
       "       [9.99481082e-01],\n",
       "       [9.40481782e-01],\n",
       "       [9.98237371e-01],\n",
       "       [9.98193741e-01],\n",
       "       [9.99459386e-01],\n",
       "       [9.98256028e-01],\n",
       "       [9.98138547e-01],\n",
       "       [9.97583270e-01],\n",
       "       [9.98248458e-01],\n",
       "       [9.99426723e-01],\n",
       "       [9.98221338e-01],\n",
       "       [9.97834980e-01],\n",
       "       [9.97812867e-01],\n",
       "       [9.98028278e-01],\n",
       "       [9.37309444e-01],\n",
       "       [5.49901187e-01],\n",
       "       [3.59237194e-04],\n",
       "       [9.96579289e-01],\n",
       "       [9.39006627e-01],\n",
       "       [9.97792721e-01],\n",
       "       [9.99271035e-01],\n",
       "       [9.36693907e-01],\n",
       "       [9.96574461e-01],\n",
       "       [9.96556163e-01],\n",
       "       [9.99457359e-01],\n",
       "       [9.96989608e-01],\n",
       "       [9.99460101e-01],\n",
       "       [9.97995019e-01],\n",
       "       [9.96448934e-01],\n",
       "       [9.96535599e-01],\n",
       "       [9.98213649e-01],\n",
       "       [9.38416362e-01],\n",
       "       [9.96453702e-01],\n",
       "       [9.98255253e-01],\n",
       "       [9.98347938e-01],\n",
       "       [9.99423265e-01],\n",
       "       [9.98079777e-01],\n",
       "       [9.98010159e-01],\n",
       "       [9.96601343e-01],\n",
       "       [9.99466300e-01],\n",
       "       [9.98244345e-01],\n",
       "       [9.98202980e-01],\n",
       "       [9.98092115e-01],\n",
       "       [9.99372900e-01],\n",
       "       [9.96232748e-01],\n",
       "       [9.99411583e-01],\n",
       "       [9.38725710e-01],\n",
       "       [9.39834476e-01],\n",
       "       [9.96625900e-01],\n",
       "       [9.39437687e-01],\n",
       "       [9.92772818e-01],\n",
       "       [9.98253584e-01],\n",
       "       [9.98265266e-01],\n",
       "       [9.98251557e-01],\n",
       "       [8.89480114e-04],\n",
       "       [9.86674547e-01],\n",
       "       [9.98058438e-01],\n",
       "       [9.98197556e-01],\n",
       "       [9.38702226e-01],\n",
       "       [9.40172434e-01],\n",
       "       [3.51428986e-04],\n",
       "       [9.99447823e-01],\n",
       "       [9.96608138e-01],\n",
       "       [9.98199582e-01],\n",
       "       [9.40164208e-01],\n",
       "       [9.99443650e-01],\n",
       "       [9.99381363e-01],\n",
       "       [9.96631503e-01],\n",
       "       [5.49901187e-01],\n",
       "       [9.99374330e-01],\n",
       "       [9.96314168e-01],\n",
       "       [9.99411464e-01],\n",
       "       [9.99475598e-01],\n",
       "       [5.49901426e-01],\n",
       "       [9.99256968e-01],\n",
       "       [9.96297479e-01],\n",
       "       [9.39358115e-01],\n",
       "       [9.37996626e-01],\n",
       "       [9.99465644e-01],\n",
       "       [9.98216152e-01],\n",
       "       [9.98145461e-01],\n",
       "       [9.98006821e-01],\n",
       "       [5.49901485e-01],\n",
       "       [9.96630549e-01],\n",
       "       [9.98096943e-01],\n",
       "       [9.99477446e-01],\n",
       "       [9.39623773e-01],\n",
       "       [9.37109828e-01],\n",
       "       [9.99480188e-01],\n",
       "       [5.49901187e-01],\n",
       "       [9.96631503e-01],\n",
       "       [9.98292565e-01],\n",
       "       [9.98144031e-01],\n",
       "       [9.98271227e-01],\n",
       "       [9.38450098e-01],\n",
       "       [9.98244941e-01],\n",
       "       [9.99454975e-01],\n",
       "       [9.98131931e-01],\n",
       "       [9.39905286e-01],\n",
       "       [9.38534737e-01],\n",
       "       [9.99431610e-01],\n",
       "       [9.99482870e-01],\n",
       "       [9.98083830e-01],\n",
       "       [9.38215852e-01],\n",
       "       [9.99486089e-01],\n",
       "       [9.99450088e-01],\n",
       "       [9.37577605e-01],\n",
       "       [5.49901247e-01],\n",
       "       [9.99348640e-01],\n",
       "       [9.98180628e-01],\n",
       "       [9.40508544e-01],\n",
       "       [9.98221159e-01],\n",
       "       [9.39493060e-01],\n",
       "       [9.98126507e-01],\n",
       "       [9.97933924e-01],\n",
       "       [9.99418378e-01],\n",
       "       [9.98242140e-01],\n",
       "       [9.99463797e-01],\n",
       "       [5.49901366e-01],\n",
       "       [9.40041542e-01],\n",
       "       [9.39223707e-01],\n",
       "       [9.99462366e-01],\n",
       "       [9.99455810e-01],\n",
       "       [9.99450207e-01],\n",
       "       [9.39725637e-01],\n",
       "       [9.97672319e-01],\n",
       "       [9.37900305e-01],\n",
       "       [9.96618748e-01],\n",
       "       [9.40699041e-01],\n",
       "       [9.99043226e-01],\n",
       "       [9.38246250e-01],\n",
       "       [9.98177886e-01],\n",
       "       [9.98023629e-01],\n",
       "       [9.96389508e-01],\n",
       "       [9.98234808e-01],\n",
       "       [9.37630177e-01],\n",
       "       [9.93894994e-01],\n",
       "       [9.98202085e-01],\n",
       "       [9.97701049e-01],\n",
       "       [9.39960837e-01],\n",
       "       [9.37052846e-01],\n",
       "       [9.99421835e-01],\n",
       "       [9.97926950e-01],\n",
       "       [9.96626019e-01],\n",
       "       [9.98147726e-01],\n",
       "       [9.98146534e-01],\n",
       "       [9.36827898e-01],\n",
       "       [2.88188457e-04],\n",
       "       [1.26829743e-02],\n",
       "       [4.41491604e-04],\n",
       "       [9.37180400e-01],\n",
       "       [7.25042522e-01],\n",
       "       [1.97180212e-02],\n",
       "       [4.28915024e-04],\n",
       "       [5.49901247e-01],\n",
       "       [4.92522120e-03],\n",
       "       [5.95003366e-04],\n",
       "       [1.90846324e-02],\n",
       "       [9.97785926e-01],\n",
       "       [5.49901426e-01],\n",
       "       [2.79545784e-04],\n",
       "       [2.77996063e-04],\n",
       "       [3.31181288e-03],\n",
       "       [5.08922338e-03],\n",
       "       [9.95000005e-01],\n",
       "       [5.49901307e-01],\n",
       "       [3.71962786e-04],\n",
       "       [2.71849036e-02],\n",
       "       [2.75164843e-04],\n",
       "       [4.51922417e-04],\n",
       "       [9.95491743e-01],\n",
       "       [9.96968389e-01],\n",
       "       [2.91377306e-04],\n",
       "       [4.90912795e-03],\n",
       "       [3.12268734e-04],\n",
       "       [4.83319163e-03],\n",
       "       [2.89320946e-04],\n",
       "       [4.12583351e-04],\n",
       "       [2.80499458e-04],\n",
       "       [4.11778688e-04],\n",
       "       [1.29214227e-02],\n",
       "       [3.73363495e-04],\n",
       "       [3.29583883e-04],\n",
       "       [1.19152963e-02],\n",
       "       [5.80298901e-03],\n",
       "       [1.31902099e-02],\n",
       "       [5.49902678e-01],\n",
       "       [1.28023028e-02],\n",
       "       [9.39128041e-01],\n",
       "       [5.49901247e-01],\n",
       "       [5.49901187e-01],\n",
       "       [8.53747129e-04],\n",
       "       [5.49901187e-01],\n",
       "       [4.07397747e-04],\n",
       "       [2.78770924e-04],\n",
       "       [2.82317400e-04],\n",
       "       [1.30584240e-02],\n",
       "       [5.49903989e-01],\n",
       "       [2.76923180e-04],\n",
       "       [9.97370422e-01],\n",
       "       [2.57940888e-02],\n",
       "       [2.91436911e-04],\n",
       "       [1.65889561e-02],\n",
       "       [9.97519851e-01],\n",
       "       [9.36553478e-01],\n",
       "       [9.37637448e-01],\n",
       "       [2.78919935e-04],\n",
       "       [6.54459000e-04],\n",
       "       [2.85357237e-04],\n",
       "       [4.81271744e-03],\n",
       "       [5.67257404e-03],\n",
       "       [4.93919849e-03],\n",
       "       [4.44084406e-04],\n",
       "       [4.10586596e-04],\n",
       "       [4.96768951e-03],\n",
       "       [9.37018156e-01],\n",
       "       [9.84864175e-01],\n",
       "       [9.83580947e-01],\n",
       "       [9.37138200e-01],\n",
       "       [9.36377168e-01],\n",
       "       [9.35764670e-01],\n",
       "       [9.80211198e-02],\n",
       "       [9.36862946e-01],\n",
       "       [9.99418139e-01],\n",
       "       [9.97385561e-01],\n",
       "       [2.77161598e-04],\n",
       "       [3.73005867e-04],\n",
       "       [3.84449959e-04],\n",
       "       [4.13030386e-04],\n",
       "       [7.37577677e-04],\n",
       "       [4.46140766e-04],\n",
       "       [5.49901366e-01],\n",
       "       [5.49901366e-01],\n",
       "       [9.36979651e-01],\n",
       "       [1.06137991e-03],\n",
       "       [1.22102797e-02],\n",
       "       [3.91483307e-04],\n",
       "       [7.09840655e-03],\n",
       "       [9.81057525e-01],\n",
       "       [9.35773849e-01],\n",
       "       [9.14493203e-03],\n",
       "       [1.38665140e-02],\n",
       "       [2.29648352e-02],\n",
       "       [9.98032868e-01],\n",
       "       [1.27847493e-02],\n",
       "       [5.87228537e-01],\n",
       "       [1.45064294e-02],\n",
       "       [1.65765882e-02],\n",
       "       [9.16514516e-01],\n",
       "       [4.89369035e-03],\n",
       "       [1.82281405e-01],\n",
       "       [4.25603688e-02],\n",
       "       [3.77714634e-04],\n",
       "       [2.94996484e-04],\n",
       "       [4.04879218e-04],\n",
       "       [5.60539961e-03],\n",
       "       [5.49901605e-01],\n",
       "       [1.96155310e-02],\n",
       "       [9.98225868e-01],\n",
       "       [9.35662210e-01],\n",
       "       [9.36049759e-01],\n",
       "       [5.73831797e-03],\n",
       "       [5.49901187e-01],\n",
       "       [9.37700033e-01],\n",
       "       [4.45221364e-02],\n",
       "       [2.52764523e-02],\n",
       "       [2.83951580e-04],\n",
       "       [2.75581900e-04],\n",
       "       [3.75747681e-04],\n",
       "       [4.83980775e-03],\n",
       "       [4.88513708e-03],\n",
       "       [1.24704540e-02],\n",
       "       [1.19605958e-02],\n",
       "       [5.49901843e-01],\n",
       "       [4.75263660e-04],\n",
       "       [4.78449464e-03],\n",
       "       [9.83112395e-01],\n",
       "       [5.49901366e-01],\n",
       "       [5.49902201e-01],\n",
       "       [6.24492764e-03],\n",
       "       [8.95562410e-01],\n",
       "       [5.96820086e-04],\n",
       "       [9.35996890e-01],\n",
       "       [1.42310560e-02],\n",
       "       [4.54393477e-04],\n",
       "       [9.60747242e-01],\n",
       "       [6.95076585e-03],\n",
       "       [1.21962428e-02],\n",
       "       [5.63545094e-04],\n",
       "       [3.85369360e-02],\n",
       "       [6.20877743e-03],\n",
       "       [4.73562133e-04],\n",
       "       [4.28927073e-04],\n",
       "       [1.28939084e-03],\n",
       "       [4.78816032e-03],\n",
       "       [5.10358810e-03],\n",
       "       [4.91631031e-03],\n",
       "       [1.00562274e-02],\n",
       "       [4.84022498e-03]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_of_fold_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out of fold accuracy        = 0.904239\n",
      "Base rate guessing accuracy = 0.759812\n"
     ]
    }
   ],
   "source": [
    "#print metrics on hold-out data\n",
    "out_of_fold_preds = np.squeeze(out_of_fold_preds)\n",
    "out_of_fold_probs = out_of_fold_preds > 0.5\n",
    "out_of_fold_acc = np.mean(out_of_fold_probs == labels)\n",
    "print(\"Out of fold accuracy        = %3f\"%out_of_fold_acc)\n",
    "print(\"Base rate guessing accuracy = %3f\"%max(np.mean(labels==0), np.mean(labels==1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#output of the model is a binary vector\n",
    "model.fit(train_matrix, train_labels, epochs = 50, batch_size = 32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pred = model.predict(train_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.sum(train_pred>0.5))\n",
    "print(np.sum(train_pred<=0.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now train with cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "n_folds = 5\n",
    "n_epochs = 10**3\n",
    "\n",
    "skf = StratifiedKFold(n_splits = n_folds, random_state = 33, shuffle=True)\n",
    "\n",
    "out_of_fold_preds = []\n",
    "true_labels = []\n",
    "words = np.array([])\n",
    "\n",
    "model_W_list = []\n",
    "model_b_list = []\n",
    "\n",
    "\n",
    "#loop over k-fold splits\n",
    "for k, (train_index, test_index) in enumerate(skf.split(embedding_matrix, labels)):\n",
    "    print('\\nTraining L2-regularized logistic regression on fold %d / %d :\\n'%(k+1, n_folds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train a model on sentences describing failures of various kinds \n",
    "(mainly engineering project / software engineering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single-word embeddings for logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#embed all words from a given piece of text\n",
    "#return a matrix: rows are words, columns are embedding dimensions\n",
    "#skips any words not in embedding\n",
    "def get_embedding(glove_df, text):\n",
    "    words = text.split()\n",
    "    single_word_embeddings = []\n",
    "    words_used = []\n",
    "    for w in words:\n",
    "        if(w in glove_df.columns):\n",
    "            single_word_embeddings.append(glove_df[w].values)\n",
    "            words_used.append(w)\n",
    "    return (np.array(single_word_embeddings), words_used)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#embed the response to the second question\n",
    "#for all interviews in the dataset\n",
    "fail_list = []\n",
    "success_list = []\n",
    "fail_words = []\n",
    "success_words = []\n",
    "for doc in data:\n",
    "    r2_embed, words = get_embedding(glove_df, doc['responses'][1])\n",
    "    if('failure' in doc['tags']):\n",
    "        fail_list.append(r2_embed)\n",
    "        fail_words = fail_words + words\n",
    "    elif('success' in doc['tags']):\n",
    "        success_list.append(r2_embed)\n",
    "        success_words = success_words + words\n",
    "        \n",
    "r2_embed_fail    = np.concatenate(fail_list, axis=0)\n",
    "r2_embed_success = np.concatenate(success_list, axis=0)\n",
    "r2_embed = np.concatenate([r2_embed_fail, r2_embed_success], axis=0)\n",
    "r2_words = np.array(fail_words + success_words)\n",
    "labels = np.array([0]*r2_embed_fail.shape[0] + [1]*r2_embed_success.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_embed.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L2-regularized Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "def train_L2_logistic_regression(X_train, Y_train, X_test=None,\\\n",
    "                                L2_lambda=0.1, learning_rate=10**(-5), n_epochs=10**3, minibatch_size=128,\\\n",
    "                                print_progress=True):\n",
    "    \n",
    "    n_train_data = X_train.shape[0]\n",
    "    n_features = X_train.shape[1]\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(50)\n",
    "    \n",
    "    #placeholders for data and parameters:\n",
    "    X_ = tf.placeholder(tf.float32, shape = [n_features, None], name=\"X\")\n",
    "    Y_ = tf.placeholder(tf.float32, shape = [None], name=\"Y\")\n",
    "    L2_lambda_ = tf.placeholder(tf.float32, shape=(), name=\"L2_lambda\")\n",
    "    \n",
    "    W_ = tf.Variable(tf.ones([1, n_features])*0.01, name=\"W\")\n",
    "    b_ = tf.Variable(tf.ones([1,1]), name=\"b\")\n",
    "    \n",
    "    #compute linear predictor \n",
    "    #W_ and X_ are vectors so matmul is actually dot product\n",
    "    Z_ = tf.matmul(W_, X_) + b_\n",
    "    Z_ = tf.squeeze(Z_)\n",
    "    \n",
    "    cost = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=tf.transpose(Z_),\\\n",
    "                                                                 labels=tf.transpose(Y_)))\\\n",
    "            + L2_lambda_*tf.norm(W_, axis=None, ord=2)\n",
    "        \n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "    \n",
    "    init = tf.global_variables_initializer()\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init)\n",
    "        epoch_cost_mean = np.ones(n_epochs)*np.nan\n",
    "        epoch_cost_sem  = np.ones(n_epochs)*np.nan\n",
    "        \n",
    "        for epoch in range(n_epochs):\n",
    "            n_minibatches = int(n_train_data/minibatch_size)\n",
    "            shuffled_indices = np.random.permutation(n_train_data)\n",
    "            minibatch_indices =\\\n",
    "            [shuffled_indices[i*minibatch_size : (i+1)*minibatch_size] for i in range(int(np.ceil(float(len(shuffled_indices))/minibatch_size)))]\n",
    "            minibatch_costs = np.ones(len(minibatch_indices))*np.nan\n",
    "            for k, ind in enumerate(minibatch_indices):\n",
    "                X_minibatch = X_train[ind,:]\n",
    "                Y_minibatch = Y_train[ind]\n",
    "                _, minibatch_cost = sess.run([optimizer, cost],\\\n",
    "                                         feed_dict = {X_:X_minibatch.T, Y_:Y_minibatch, L2_lambda_:np.array(L2_lambda)})\n",
    "                minibatch_costs[k] = minibatch_cost\n",
    "\n",
    "            epoch_cost_mean[epoch] = np.mean(minibatch_costs)\n",
    "            epoch_cost_sem[epoch]  = np.std(minibatch_costs)/np.sqrt(len(minibatch_costs))\n",
    "\n",
    "            if(epoch % 100 ==0):\n",
    "\n",
    "                print(\"Cost after epoch %d = %f\"%(epoch, epoch_cost_mean[epoch]))\n",
    "        #end loop over epochs\n",
    "\n",
    "        #now get predictions\n",
    "        probs_train = 1.0/(1.0 + np.exp(-Z_.eval({X_:X_train.T})))\n",
    "\n",
    "        if X_test is not None:\n",
    "            probs_test  = 1.0/(1.0 + np.exp(-Z_.eval({X_:X_test.T})))\n",
    "        else:\n",
    "            probs_test = None\n",
    "\n",
    "        #extract parameters of trained model\n",
    "        model = {\"W\":W_.eval(), \"b\":b_.eval()}\n",
    "\n",
    "    if(X_test is not None):\n",
    "        return (probs_train, probs_test, model)\n",
    "    else:\n",
    "        return (probs_train, model)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit L2-regularized logistic regression via tensorflow\n",
    "#this achieves two things: gives a baseline we can compare more complicated models to\n",
    "# and yields a separating hyperplane in semantic space\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "n_folds = 5\n",
    "n_epochs = 10**3\n",
    "\n",
    "skf = StratifiedKFold(n_splits = n_folds, random_state = 33, shuffle=True)\n",
    "\n",
    "out_of_fold_preds = []\n",
    "true_labels = []\n",
    "words = np.array([])\n",
    "\n",
    "model_W_list = []\n",
    "model_b_list = []\n",
    "\n",
    "\n",
    "#loop over k-fold splits\n",
    "for k, (train_index, test_index) in enumerate(skf.split(r2_embed, labels)):\n",
    "    print('\\nTraining L2-regularized logistic regression on fold %d / %d :\\n'%(k+1, n_folds))\n",
    "    X_train = r2_embed[train_index,:]\n",
    "    Y_train = labels[train_index]\n",
    "    X_test = r2_embed[test_index,:]\n",
    "    Y_test = labels[test_index]\n",
    "    probs_train, probs_test, model = train_L2_logistic_regression(X_train, Y_train, X_test, n_epochs=n_epochs)\n",
    "    out_of_fold_preds.append(probs_test)\n",
    "    true_labels.append(Y_test)\n",
    "    words = np.concatenate([words, r2_words[test_index]])\n",
    "    model_W_list.append(model['W'])\n",
    "    model_b_list.append(model['b'])\n",
    "#concatenate results into single vectors: out-of-fold prediction for each word\n",
    "out_of_fold_preds = np.concatenate(out_of_fold_preds, axis=0)\n",
    "true_labels       = np.concatenate(true_labels, axis=0)\n",
    "loss = out_of_fold_preds*true_labels + (1 - out_of_fold_preds)*(1 - true_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_W = np.concatenate(model_W_list, axis=0)\n",
    "model_b = np.concatenate(model_b_list, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_W.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_W = np.mean(model_W, axis=0)\n",
    "model_scores = glove_df.apply(lambda x: np.dot(x, mean_W)/np.linalg.norm(x,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_scores = model_scores.sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_scores[0:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot results\n",
    "import matplotlib.pyplot as plt\n",
    "lin_pred = -np.log(1.0/out_of_fold_preds-1)\n",
    "lin_pred_pos = lin_pred[[true_labels[k]==1 for k in range(len(true_labels))]]]\n",
    "lin_pred_neg = lin_pred[[true_labels[k]==0 for k in range(len(true_labels))]]]\n",
    "\n",
    "bin_width = 0.25\n",
    "bins = bp.arange(-8,8,bin_width)\n",
    "hist_pos = np.histogram(lin_pred_pos, bins)\n",
    "hist_neg = np.histogram(lin_pred_neg, bins)\n",
    "\n",
    "centers = lambda b : (b[1:] + b[:-1])/2.0\n",
    "\n",
    "plt.figure(figsize=[12,7])\n",
    "plt.bar(centers(hist_neg[1]), hist_neg[0], width=bin_width, align='center', color=[1,0,0.7],alpha=0.5,label='words from failure articles')\n",
    "plt.bar(centers(hist_pos[1]), hist_pos[0], width=bin_width, align='center', color=[0,0.8,0.8],alpha=1,label='words from success articles')\n",
    "\n",
    "plt.gca().set_yticklabels([u.astype(int) for u in plt.gca().get_yticks()], fontsize=16)\n",
    "plt.gca().set_xticklabels([u.astype(int) for u in plt.gca().get_xticks()], fontsize=16)\n",
    "\n",
    "plt.legend(fontsize=20)\n",
    "plt.xlabel('Linear Score', fontsize=20, labelpad=20)\n",
    "plt.plot([0,0],[0,100],'--k',linewidth=1.5)\n",
    "plt.ylim([0,80])\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print best-classified words\n",
    "n_to_print = 50\n",
    "order = np.argsort(accuracy)\n",
    "\n",
    "print('Best-classified failure words:')\n",
    "failure_words = words[order][true_labels[order]==0][:n_to_print]\n",
    "print(failure_words)\n",
    "success_words = words[order][true_labels[order]==1][:n_to_print]\n",
    "print(success_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot startup locations on a map\n",
    "\n",
    "import gmplot\n",
    "\n",
    "\n",
    "\n",
    "gmap = gmplot.GoogleMapPlotter(37.428, -50, 2)\n",
    "\n",
    "if(googlemaps_api_key):\n",
    "    gmap.apikey=googlemaps_api_key\n",
    "\n",
    "#gmap.plot(latitudes, longitudes, 'cornflowerblue', edge_width=10)\n",
    "#gmap.scatter(more_lats, more_lngs, '#3B0B39', size=40, marker=False)\n",
    "#gmap.scatter(marker_lats, marker_lngs, 'k', marker=True)\n",
    "#gmap.heatmap(heat_lats, heat_lngs)\n",
    "\n",
    "gmap.draw(\"mymap.html\")\n",
    "from IPython.display import IFrame\n",
    "\n",
    "IFrame(src='./mymap.html', width=700, height=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with output_f as open('output.txt', 'w'):\n",
    "    output_f.write('stuff')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
