{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detecting descriptions of failure in text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this project is to build a system that can detect parts of text that describe instances of failure - such as failure of a project, a piece of equipment, or a company. This problem resembles sentiment analysis, and I will be using some approaches from sentiment analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Table of contents\n",
    "\n",
    "This notebook is organized into sections as follows:\n",
    "    1. Assemble training data : assemble a collection of example sentences used to train the classifier. \n",
    "                                Positive sentences (those describing failure) are loaded directly from a file. \n",
    "                                Negative sentences (no failure) are extracted from Wikipedia articles \n",
    "                                after some text cleaning.\n",
    "    2. Train the classifier   : load GloVe word embedding data. Train a LSTM network on the example sentences,\n",
    "                                with words replaced by vectors according to the word embedding. Use crossvalidation  \n",
    "                                to obtain an estimate of how well the classifier can generalize on data drawn from \n",
    "                                the training distribution.\n",
    "    3. Assemble test data     : use web scraping to gather a dataset of interviews with startup founders from \n",
    "                                www.failory.com. The startups either succeeded or failed, we want to use the pre-\n",
    "                                trained classifier to try to determine what happened. \n",
    "                                3a : scrape main pages and download all the relevant articles, save HTML to sqllite \n",
    "                                     database. Parse HTML to extract text of interest, save processed text to .json.\n",
    "                                3b : use googlemaps API to plot a map showing locations of all startups in dataset\n",
    "    4. Classify test data     : run the classifier trained in 2. on the failory.com dataset \n",
    "    \n",
    "All intermediate results are saved to file at the end of each section and re-loaded at the start of the next section,\n",
    "so it is possible to start running the notebook at any section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#package installation for jupyter\n",
    "#if any packages are missing, they can be installed and made available for jupyter \n",
    "#by running the code below directly in the notebook (only need to do this once)\n",
    "#import sys\n",
    "#!{sys.executable} -m pip install keras\n",
    "#!{sys.executable} -m pip install beautifulsoup4\n",
    "#!{sys.executable} -m pip install requests\n",
    "#!{sys.executable} -m pip install  ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Assemble training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step is to assemble a set of labeled text data for training the algorithm. I plan to use an algorithm that takes single sentences or parts of sentences as input, and returns an estimated probability that the sentence describes an instance of failure. Therefore, I need a training dataset consisting of sentences with binary labels. I refer to these sentences as positive cases if they describe failure, and negative cases if they do not.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To acquire the positive cases (sentences describing failure), I manually extracted sentences from a variety of texts. These included descriptions of failed construction projects, failed software engineering projects, failed charitable initiatives, failed startups, and other instances of failure. The websites included Medium, Quora, calleam.com and several others. Using multiple sources is crucial, because it helps to prevent the algorithm from learning any spurious associations between the language style of a sentence and its failure-related status. A full list of sources is given in the file of positive cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import sqlite3\n",
    "import requests\n",
    "import urllib.request\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load positive cases (sentences describing an instance of failure)\n",
    "#these sentences are accumulated as keys in a python dict that is written to a file as raw code\n",
    "#(so it is easy to modify directly)\n",
    "\n",
    "from training_positive_cases import training_positive_cases\n",
    "training_positive_cases = list(training_positive_cases.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obamacare Website Programmers Complained About Unrealistic Deadlines\n",
      "\n",
      "Ignoring users is a tried and true way to fail\n",
      "\n",
      "the President had to admit that the performance of the system was below what would be expected\n",
      "\n",
      "Despite significant technical problems with the prototype\n",
      "\n",
      "they never spend any money promoting it and it goes unused and is left to die\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of positive training cases = %d\\n\\n\"%len(training_positive_cases))\n",
    "#print a few cases at random\n",
    "print(\"Examples:\\n\\n\")\n",
    "_=[print(s + \"\\n\") for s in np.array(training_positive_cases)[np.random.permutation(len(training_positive_cases))[0:5]]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The negative cases are sentences/parts of sentences that do not describe failure, so these belong to a much larger and more diverse set. They need to resemble the positive cases in terms of style, general language use, and non-failure-related vocabulary, because if there is any systematic difference the algorithm could learn a spurious associations. To obtain the negative cases, I downloaded the text of multiple Wikipedia articles on specific software and other projects which are not known for failure, and used all sentences from the main body of text of these articles as negative cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "downloading https://en.wikipedia.org/wiki/Linux\n",
      "\n",
      "\n",
      "downloading https://en.wikipedia.org/wiki/Triborough_Bridge\n",
      "\n",
      "\n",
      "downloading https://en.wikipedia.org/wiki/Database\n",
      "\n",
      "\n",
      "downloading https://en.wikipedia.org/wiki/Lean_startup\n",
      "\n",
      "\n",
      "downloading https://en.wikipedia.org/wiki/Business_model\n",
      "\n",
      "\n",
      "downloading https://en.wikipedia.org/wiki/Pinterest\n",
      "\n",
      "\n",
      "downloading https://en.wikipedia.org/wiki/Twitter\n",
      "\n",
      "\n",
      "downloading https://en.wikipedia.org/wiki/Application_software\n",
      "\n",
      "\n",
      "downloading https://en.wikipedia.org/wiki/Web_search_engine\n",
      "\n",
      "\n",
      "downloading https://en.wikipedia.org/wiki/Software\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from training_negative_urls import training_negative_urls\n",
    "\n",
    "#download the full text from the specified URLs\n",
    "#and save it to sqlite database\n",
    "#reasons: \n",
    "#         1) avoid downloading multiple times \n",
    "#         2) now have a working snapshot, will not be affected by future wikipedia edits\n",
    "conn = sqlite3.connect('negative_raw_html.sqlite')\n",
    "cur = conn.cursor()\n",
    "cur.execute('CREATE TABLE IF NOT EXISTS Negative (url TEXT, text TEXT) ')\n",
    "\n",
    "def insert_negative_text(cur, url, text):\n",
    "    cur.execute('SELECT text FROM Negative WHERE url = ?', (url,))\n",
    "    #? -> avoid SQL injection\n",
    "    row = cur.fetchone()\n",
    "    if(row is None):\n",
    "        cur.execute('''INSERT INTO Negative (url, text)\n",
    "            VALUES (?, ?)''', (url, text))\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "for url in training_negative_urls.keys():\n",
    "    print('\\ndownloading '+url+'\\n')\n",
    "    article = requests.get(url)\n",
    "    time.sleep(1)  \n",
    "    insert_negative_text(cur, url, article.text)\n",
    "    conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def clean_wikipedia_text(article_text):\n",
    "\n",
    "    soup = BeautifulSoup(article_text, \"html.parser\")\n",
    "    text = \"\"\n",
    "    tags = soup.findAll(\"p\")\n",
    "    for t in tags:\n",
    "        text = text + t.text\n",
    "    #only take contents of <p> tags\n",
    "    #this ensures we only take the main text while discarding extraneous material \n",
    "    #(references etc.)\n",
    "        \n",
    "    sentences = re.split('\\. |\\.\\\\n|\\.\\[\\d+\\]', text)\n",
    "    #split on: period followed by space | period followed by line break | period followed by citation \n",
    "\n",
    "    sentences = [s.split() for s in sentences]\n",
    "    #split on whitespace\n",
    "    sentences = [s for s in sentences if len(s) > 2 and len(s) < 50]\n",
    "    #remove unsually short or long sentences\n",
    "\n",
    "    remove_citations = lambda s : [t for t in s if \"[\" not in t and \"]\" not in t]\n",
    "    sentences = [remove_citations(s) for s in sentences]\n",
    "    \n",
    "    remove_listens = lambda s : [t for t in s if \"/\" not in t and not t==\"(listen)\"]\n",
    "    sentences = [remove_listens(s) for s in sentences]\n",
    "    \n",
    "    #remove other extraneous punctuation?\n",
    "    \n",
    "    sentences = [\" \".join(s) for s in sentences]\n",
    "    \n",
    "    return sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load texts from database into list of dicts (database_dict_list)\n",
    "database_dict_list = []\n",
    "sqlstr = 'SELECT url, text FROM Negative'\n",
    "for row in cur.execute(sqlstr):\n",
    "    entry = {}\n",
    "    entry['url'] = row[0]\n",
    "    entry['text'] = row[1]\n",
    "    database_dict_list.append(entry)\n",
    "texts = [d['text'] for d in database_dict_list]\n",
    "training_negative_cases = []\n",
    "for t in texts: training_negative_cases = training_negative_cases + clean_wikipedia_text(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For E-ZPass users, sensors detect their transponders wirelessly\n",
      "\n",
      "The Korg OASYS, the Korg KRONOS, the Yamaha Motif XF music Yamaha Yamaha synthesizers, Yamaha Motif-Rack XS tone generator module, and Roland RD-700GX digital piano also run Linux\n",
      "\n",
      "Separately, the Board of Estimate voted to create an authority to impose toll charges on both crossings\n",
      "\n",
      "Larry Ellison's Oracle Database (or more simply, Oracle) started from a different chain, based on IBM's papers on System R\n",
      "\n",
      "Many other open-source software projects contribute to Linux systems\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of negative training cases = %d\\n\\n\"%len(training_positive_cases))\n",
    "#print a few cases at random\n",
    "print(\"Examples:\\n\\n\")\n",
    "#print a few cases at random\n",
    "_=[print(s + \"\\n\") for s in np.array(training_negative_cases)[np.random.permutation(len(training_positive_cases))[0:5]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save both positive and negative cases to .json\n",
    "#so we can immediately load them and start at 2. if desired\n",
    "training_dict = {\"negatives\":training_negative_cases, \"positives\":training_positive_cases}\n",
    "with open('training_data.json', 'w') as fp:\n",
    "    json.dump(training_dict, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Train the classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I want to develop a classifier which takes sentences or parts of sentences as input and returns an estimated probability that this input describes an instance of failure.\n",
    "My approach is as follows:\n",
    "First, I use GloVe word embeddings to convert each word of the sentence to a d-dimensional vector (where d will be some value between 50-300). The idea behind this is that the embeddings should capture some aspects of the meaning of each word, which will enable the classifier to generalize to sentences with similar semantics, even if it has never seen the precise words before. For example, I would hope that once the classifier learns that the sentence \"it was a disaster\" is an instance of failure, it will subsequently classify \"it was a catastrophe\" as failure as well, even if the word \"catastrophe\" never appeared in the training dataset.\n",
    "Using word embeddings is crucial because I can't realistically assemble a training dataset that includes all combinations of relevant english words, so I need to build a system that can perform semantic generalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "import LSTM_functions as lstm\n",
    "#separate file contains functions for defining and training lstm\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "import importlib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the data that we assembled in 1. from .json\n",
    "with open('training_data.json', 'r') as fp:\n",
    "    training_data = json.load(fp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_pretrained_embeddings_path = '/users/cstoneki/Documents/analysis/general_resources/glove.6B/glove.6B.300d.txt'\n",
    "#glove_pretrained_embeddings_path = '/mnt/glove.6B.50d.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 words in glove dataset\n",
      "Words loaded : 000000 \n",
      "Words loaded : 050000 \n",
      "Words loaded : 100000 \n",
      "Words loaded : 150000 \n",
      "Words loaded : 200000 \n",
      "Words loaded : 250000 \n",
      "Words loaded : 300000 \n",
      "Words loaded : 350000 \n",
      "Finished loading data\n"
     ]
    }
   ],
   "source": [
    "#load GloVe data\n",
    "#this can take a bit of time, especially for the higher-dimensional datasets (such as 300d)\n",
    "#so report progress\n",
    "\n",
    "with open(glove_pretrained_embeddings_path) as f:\n",
    "    n_entries = 0\n",
    "    d = 0\n",
    "    \n",
    "    for k, line in enumerate(f.readlines()):\n",
    "        n_entries = k + 1\n",
    "        #the first entry is \"the\", it is well formatted\n",
    "        if(k==0): d = len(line.split()) - 1\n",
    "    glove_data = np.zeros([d, n_entries])\n",
    "    words = []\n",
    "    #store each entry (word) as column\n",
    "    print('Found %d words in glove dataset'%n_entries)\n",
    "    f.seek(0)\n",
    "    for k, line in enumerate(f.readlines()):\n",
    "        lst = line.split()\n",
    "        words.append(lst[0])\n",
    "        vals = np.array([float(s) for s in lst[1:]])\n",
    "        glove_data[:,k] = vals\n",
    "        if(k % 50000==0):\n",
    "            print('Words loaded : %06d '%k)\n",
    "    print('Finished loading data')\n",
    "        \n",
    "glove_df = pd.DataFrame(glove_data, columns=words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model I use is an LSTM, implemented using Keras. The code is in a separate file (LSTM_functions.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training LSTM on fold 1 / 5 :\n",
      "\n",
      "Epoch 1/50\n",
      "1634/1634 [==============================] - 12s 7ms/step - loss: 0.7059 - acc: 0.5184\n",
      "Epoch 2/50\n",
      "1634/1634 [==============================] - 4s 2ms/step - loss: 0.7001 - acc: 0.4982\n",
      "Epoch 3/50\n",
      "1634/1634 [==============================] - 5s 3ms/step - loss: 0.6008 - acc: 0.6854\n",
      "Epoch 4/50\n",
      "1634/1634 [==============================] - 4s 3ms/step - loss: 0.4047 - acc: 0.8409\n",
      "Epoch 5/50\n",
      "1634/1634 [==============================] - 4s 2ms/step - loss: 0.3283 - acc: 0.8733\n",
      "Epoch 6/50\n",
      "1634/1634 [==============================] - 4s 2ms/step - loss: 0.2643 - acc: 0.8953\n",
      "Epoch 7/50\n",
      "1634/1634 [==============================] - 4s 2ms/step - loss: 0.2277 - acc: 0.9137\n",
      "Epoch 8/50\n",
      "1634/1634 [==============================] - 4s 2ms/step - loss: 0.2036 - acc: 0.9296\n",
      "Epoch 9/50\n",
      "1634/1634 [==============================] - 4s 2ms/step - loss: 0.1768 - acc: 0.9327\n",
      "Epoch 10/50\n",
      "1634/1634 [==============================] - 4s 2ms/step - loss: 0.1402 - acc: 0.9486\n",
      "Epoch 11/50\n",
      "1634/1634 [==============================] - 4s 2ms/step - loss: 0.1231 - acc: 0.9565\n",
      "Epoch 12/50\n",
      "1634/1634 [==============================] - 4s 2ms/step - loss: 0.1218 - acc: 0.9559\n",
      "Epoch 13/50\n",
      "1634/1634 [==============================] - 4s 2ms/step - loss: 0.1275 - acc: 0.9590\n",
      "Epoch 14/50\n",
      "1634/1634 [==============================] - 4s 2ms/step - loss: 0.0883 - acc: 0.9694\n",
      "Epoch 15/50\n",
      "1634/1634 [==============================] - 4s 3ms/step - loss: 0.0581 - acc: 0.9847\n",
      "Epoch 16/50\n",
      "1634/1634 [==============================] - 4s 2ms/step - loss: 0.0598 - acc: 0.9853\n",
      "Epoch 17/50\n",
      "1634/1634 [==============================] - 4s 2ms/step - loss: 0.0632 - acc: 0.9853\n",
      "Epoch 18/50\n",
      "1634/1634 [==============================] - 4s 3ms/step - loss: 0.0646 - acc: 0.9835\n",
      "Epoch 19/50\n",
      "1634/1634 [==============================] - 4s 3ms/step - loss: 0.0398 - acc: 0.9927\n",
      "Epoch 20/50\n",
      "1634/1634 [==============================] - 4s 2ms/step - loss: 0.0290 - acc: 0.9951\n",
      "Epoch 21/50\n",
      "1634/1634 [==============================] - 4s 3ms/step - loss: 0.0294 - acc: 0.9945\n",
      "Epoch 22/50\n",
      "1634/1634 [==============================] - 4s 2ms/step - loss: 0.0399 - acc: 0.9902\n",
      "Epoch 23/50\n",
      "1634/1634 [==============================] - 4s 2ms/step - loss: 0.0325 - acc: 0.9933\n",
      "Epoch 24/50\n",
      "1634/1634 [==============================] - 4s 3ms/step - loss: 0.0210 - acc: 0.9963\n",
      "Epoch 25/50\n",
      "1634/1634 [==============================] - 4s 3ms/step - loss: 0.0240 - acc: 0.9951\n",
      "Epoch 26/50\n",
      "1634/1634 [==============================] - 4s 3ms/step - loss: 0.0195 - acc: 0.9957\n",
      "Epoch 27/50\n",
      "1634/1634 [==============================] - 4s 3ms/step - loss: 0.0235 - acc: 0.9957\n",
      "Epoch 28/50\n",
      "1634/1634 [==============================] - 4s 2ms/step - loss: 0.0245 - acc: 0.9939\n",
      "Epoch 29/50\n",
      "1634/1634 [==============================] - 4s 2ms/step - loss: 0.0461 - acc: 0.9908\n",
      "Epoch 30/50\n",
      "1634/1634 [==============================] - 4s 2ms/step - loss: 0.0710 - acc: 0.9853\n",
      "Epoch 31/50\n",
      "1634/1634 [==============================] - 4s 2ms/step - loss: 0.0416 - acc: 0.9890\n",
      "Epoch 32/50\n",
      "1634/1634 [==============================] - 4s 2ms/step - loss: 0.0197 - acc: 0.9957\n",
      "Epoch 33/50\n",
      "1634/1634 [==============================] - 4s 3ms/step - loss: 0.0317 - acc: 0.9933\n",
      "Epoch 34/50\n",
      "1634/1634 [==============================] - 4s 3ms/step - loss: 0.1248 - acc: 0.9682\n",
      "Epoch 35/50\n",
      "1634/1634 [==============================] - 4s 2ms/step - loss: 0.0332 - acc: 0.9908\n",
      "Epoch 36/50\n",
      "1634/1634 [==============================] - 4s 3ms/step - loss: 0.0192 - acc: 0.9963\n",
      "Epoch 37/50\n",
      "1634/1634 [==============================] - 4s 3ms/step - loss: 0.0106 - acc: 0.9988\n",
      "Epoch 38/50\n",
      "1634/1634 [==============================] - 4s 2ms/step - loss: 0.0337 - acc: 0.9951\n",
      "Epoch 39/50\n",
      "1634/1634 [==============================] - 4s 2ms/step - loss: 0.0470 - acc: 0.9914\n",
      "Epoch 40/50\n",
      "1634/1634 [==============================] - 4s 2ms/step - loss: 0.0398 - acc: 0.9933\n",
      "Epoch 41/50\n",
      "1634/1634 [==============================] - 4s 2ms/step - loss: 0.0327 - acc: 0.9914\n",
      "Epoch 42/50\n",
      "1634/1634 [==============================] - 4s 2ms/step - loss: 0.0144 - acc: 0.9969\n",
      "Epoch 43/50\n",
      "1634/1634 [==============================] - 4s 2ms/step - loss: 0.0101 - acc: 0.9988\n",
      "Epoch 44/50\n",
      "1634/1634 [==============================] - 4s 3ms/step - loss: 0.0089 - acc: 0.9988\n",
      "Epoch 45/50\n",
      "1634/1634 [==============================] - 5s 3ms/step - loss: 0.0083 - acc: 0.9988\n",
      "Epoch 46/50\n",
      "1634/1634 [==============================] - 5s 3ms/step - loss: 0.0088 - acc: 0.9988\n",
      "Epoch 47/50\n",
      "1634/1634 [==============================] - 4s 3ms/step - loss: 0.0103 - acc: 0.9988\n",
      "Epoch 48/50\n",
      "1634/1634 [==============================] - 4s 2ms/step - loss: 0.0088 - acc: 0.9988\n",
      "Epoch 49/50\n",
      "1634/1634 [==============================] - 4s 2ms/step - loss: 0.0073 - acc: 0.9988\n",
      "Epoch 50/50\n",
      "1634/1634 [==============================] - 4s 2ms/step - loss: 0.0073 - acc: 0.9988\n",
      "\n",
      "Training LSTM on fold 2 / 5 :\n",
      "\n",
      "Epoch 1/50\n",
      "1634/1634 [==============================] - 10s 6ms/step - loss: 0.7053 - acc: 0.5043\n",
      "Epoch 2/50\n",
      "1634/1634 [==============================] - 4s 2ms/step - loss: 0.7072 - acc: 0.4945\n",
      "Epoch 3/50\n",
      "1634/1634 [==============================] - 4s 3ms/step - loss: 0.6980 - acc: 0.4982\n",
      "Epoch 4/50\n",
      "1634/1634 [==============================] - 4s 3ms/step - loss: 0.6394 - acc: 0.6230\n",
      "Epoch 5/50\n",
      "1634/1634 [==============================] - 4s 3ms/step - loss: 0.6576 - acc: 0.6151\n",
      "Epoch 6/50\n",
      "1634/1634 [==============================] - 4s 3ms/step - loss: 0.5927 - acc: 0.6793\n",
      "Epoch 7/50\n",
      "1634/1634 [==============================] - 4s 3ms/step - loss: 0.5303 - acc: 0.7803\n",
      "Epoch 8/50\n",
      "1634/1634 [==============================] - 4s 3ms/step - loss: 0.6122 - acc: 0.6946\n",
      "Epoch 9/50\n",
      "1634/1634 [==============================] - 4s 2ms/step - loss: 0.4817 - acc: 0.7809\n",
      "Epoch 10/50\n",
      "1634/1634 [==============================] - 4s 2ms/step - loss: 0.5294 - acc: 0.7717\n",
      "Epoch 11/50\n",
      "1634/1634 [==============================] - 4s 2ms/step - loss: 0.6673 - acc: 0.5673\n",
      "Epoch 12/50\n",
      "1634/1634 [==============================] - 4s 2ms/step - loss: 0.6546 - acc: 0.6022\n",
      "Epoch 13/50\n",
      "1634/1634 [==============================] - 4s 3ms/step - loss: 0.5160 - acc: 0.7723\n",
      "Epoch 14/50\n",
      "1634/1634 [==============================] - 4s 3ms/step - loss: 0.4439 - acc: 0.8195\n",
      "Epoch 15/50\n",
      "1634/1634 [==============================] - 4s 2ms/step - loss: 0.3773 - acc: 0.8458\n",
      "Epoch 16/50\n",
      "1634/1634 [==============================] - 4s 2ms/step - loss: 0.3295 - acc: 0.8727\n",
      "Epoch 17/50\n",
      "1634/1634 [==============================] - 4s 3ms/step - loss: 0.2905 - acc: 0.9021\n",
      "Epoch 18/50\n",
      "1634/1634 [==============================] - 4s 3ms/step - loss: 0.2888 - acc: 0.8996\n",
      "Epoch 19/50\n",
      "1634/1634 [==============================] - 4s 2ms/step - loss: 0.2500 - acc: 0.9149\n",
      "Epoch 20/50\n",
      "1634/1634 [==============================] - 4s 2ms/step - loss: 0.2304 - acc: 0.9266\n",
      "Epoch 21/50\n",
      "1634/1634 [==============================] - 4s 3ms/step - loss: 0.2265 - acc: 0.9278\n",
      "Epoch 22/50\n",
      "1634/1634 [==============================] - 4s 3ms/step - loss: 0.2190 - acc: 0.9339\n",
      "Epoch 23/50\n",
      "1634/1634 [==============================] - 4s 2ms/step - loss: 0.2102 - acc: 0.9333\n",
      "Epoch 24/50\n",
      " 832/1634 [==============>...............] - ETA: 1s - loss: 0.1877 - acc: 0.9411"
     ]
    }
   ],
   "source": [
    "importlib.reload(lstm)\n",
    "hp = lstm.get_default_hyperparameters()\n",
    "#train_LSTM: \n",
    "# inputs are list of positive sentences, list of negative sentences, embedding mapping dataframe, and hyperparameter dictionary (optional)\n",
    "# outputs are trained model, out-of-fold predictions from crossvalidation, true labels, and list of sentences actually used for training\n",
    "# (depending on hyperparameters, may not use all the training data provided)\n",
    "\n",
    "(model, out_of_fold_preds, labels, training_cases) = lstm.train_LSTM(training_data['positives'], training_data['negatives'], glove_df,hp=hp)\n",
    "#pass default hyperparameters\n",
    "#so in particular, negative_positive_ratio = 0.5 -> \n",
    "#            the model will use all positive cases, and an equal number of negative cases chosen at random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of model          = 0.881115\n",
      "Baseline guessing accuracy = 0.500000\n"
     ]
    }
   ],
   "source": [
    "model_accuracy = np.mean((1.0*(np.squeeze(out_of_fold_preds)>0.5))==labels)\n",
    "baseline_guessing_accuracy = max(np.mean(labels==0), np.mean(labels==1))\n",
    "print(\"Accuracy of model          = %3f\"%model_accuracy)\n",
    "print(\"Baseline guessing accuracy = %3f\"%baseline_guessing_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The out-of-fold accuracy is substantially greater than 0.5, so the model can learn to generalize well for data drawn from the training distribution. The next step is to try to use the model to solve an actual prediction problem, using data that are drawn from a different distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Analyze startup founder interviews from failory.com "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an interesting real-world problem, I want to take a set of interviews with startup founders and determine whether the startup failed or succeeded. The interviews are collected at www.failory.com. This is a potentially challenging problem because it requires the model to deal with the semantics of the text. There are no obvious shortcuts: the interview questions are similar or identical for both failure and success, and the overall language use and vocabulary are similar in both cases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we have to gather the data from failory, using web scraping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3a. Gather Failory data using web scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_urls = {'failory failure':'https://www.failory.com/interview-failure',\n",
    "             'failory success':'https://www.failory.com/interview-success'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_report(cur,  url, text, tags):\n",
    "    cur.execute('SELECT text FROM Startups WHERE url = ?', (url,))\n",
    "    #? -> avoid SQL injection\n",
    "    row = cur.fetchone()\n",
    "    if(row is None):\n",
    "        cur.execute('''INSERT INTO Startups (url, text, tags)\n",
    "            VALUES (?, ?, ?)''', (url, text, tags))\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = sqlite3.connect('startups_03.sqlite')\n",
    "cur = conn.cursor()\n",
    "cur.execute('CREATE TABLE IF NOT EXISTS Startups (url TEXT, text TEXT, tags TEXT) ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tags, url in main_urls.items():\n",
    "    print('\\ncollecting articles from '+url+'\\n')\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    a_tags = soup.findAll('a')\n",
    "    sub_urls = []\n",
    "    for i in range(len(a_tags)):\n",
    "        try:\n",
    "            if(a_tags[i][\"class\"][0] =='card-for-interviews-title'):\n",
    "                sub_urls.append(a_tags[i][\"href\"])\n",
    "        except:\n",
    "            continue\n",
    "    for sub_url in sub_urls:\n",
    "        full_url = 'https://www.failory.com' + sub_url\n",
    "        article  = requests.get(full_url)\n",
    "        print('downloaded '+full_url)\n",
    "        time.sleep(1)\n",
    "        \n",
    "        insert_report(cur, full_url, article.text, tags)\n",
    "        conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check contents of database\n",
    "#by retrieving small text fields, not full text\n",
    "sqlstr = 'SELECT url, tags FROM Startups'\n",
    "database_dict = {}\n",
    "for row in cur.execute(sqlstr):\n",
    "    database_dict[str(row[0])] = [row[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print text of first article\n",
    "sqlstr = 'SELECT url, text, tags FROM Startups'\n",
    "for k, row in enumerate(cur.execute(sqlstr)):\n",
    "    if(k > 0): break\n",
    "    soup = BeautifulSoup(row[1], \"html.parser\")\n",
    "    print(soup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to figure out how to extract the text of the article from the mess of HTML. We need to strip out all of the ads and repeated quotes. One key part will be extracting the interviewer's questions, and the response that follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#failory has tags at the start of each article\n",
    "#these are: location, area, failure cause #1, failure cause #2\n",
    "#these are obviously extremely useful, so we want to extract them\n",
    "#try to find location\n",
    "def get_failory_tags(text):\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    article_tags = []\n",
    "    div_tags = soup.findAll('div')\n",
    "    for i in range(len(div_tags)):\n",
    "        try:\n",
    "            if(div_tags[i][\"class\"][0] ==\"secondary-tag-interview\"):\n",
    "                if(div_tags[i].text):\n",
    "                    article_tags.append(div_tags[i].text)\n",
    "        except:\n",
    "            continue\n",
    "    return article_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "database_dict_list = []\n",
    "sqlstr = 'SELECT url, text, tags FROM Startups'\n",
    "for row in cur.execute(sqlstr):\n",
    "    entry = {}\n",
    "    entry['url'] = row[0]\n",
    "    entry['text'] = row[1]\n",
    "    entry['tags'] = row[2]\n",
    "    database_dict_list.append(entry)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for entry in database_dict_list:\n",
    "    if('failory' in entry['tags']):\n",
    "        #the following will only work for failory articles\n",
    "        #so check because we may have non-failory articles in database later\n",
    "        entry['failory_tags'] = get_failory_tags(entry['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#now try to extract text of interest\n",
    "def get_questions_responses(text):\n",
    "\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    tags = soup.findAll(['h4', 'p'])\n",
    "    tags_clean = []\n",
    "    for i in range(len(tags)):\n",
    "        try:\n",
    "            if(tags[i][\"class\"][0]):\n",
    "                continue\n",
    "        except:\n",
    "            tags_clean.append(tags[i])\n",
    "        \n",
    "    questions = []\n",
    "    responses = []\n",
    "    current_text = []\n",
    "    current_question = \"\"\n",
    "    for i in range(len(tags_clean)):\n",
    "        if(tags_clean[i].name=='h4'):\n",
    "            if(current_question):\n",
    "                questions.append(current_question)\n",
    "                responses.append(\" \".join(current_text))\n",
    "            current_question = tags_clean[i].text\n",
    "            current_text = []\n",
    "        else:\n",
    "            current_text.append(tags_clean[i].text)\n",
    "            \n",
    "    return (questions, responses)\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for entry in database_dict_list:\n",
    "    q,r = get_questions_responses(entry['text'])\n",
    "    entry['questions'] = q\n",
    "    entry['responses'] = r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've spent a bit of computational time developing database_dict_list, so save it as json. Use json rather than sql a) because it's much easier to handle fields that are lists of variable length and b) because we are not growing the data entry-by-entry, but dumping a single finished database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open('failory_data.json', 'w') as fp:\n",
    "    json.dump(database_dict_list, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3b Plot a world map showing the locations of all startups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a bit of data visualization that is not essential for the classification analysis.\n",
    "www.failory.com has interviews with startup founders from a variety of countries. To visualize this better, let's show a world map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "googlemaps_api_key = \"not a valid key\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_list = [entry['failory_tags'][0] for entry in database_dict_list if 'failory_tags' in entry]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(set(country_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from geopy.geocoders import Nominatim\n",
    "geolocator = Nominatim(user_agent=\"startup-analysis\")\n",
    "\n",
    "#get coordinates for each country\n",
    "locations = []\n",
    "scales = []\n",
    "for c in list(set(country_list)):\n",
    "    \n",
    "    location = geolocator.geocode(c)\n",
    "    scales.append(int(np.sum(np.array(country_list)==c)))\n",
    "    entry = {}\n",
    "    entry['latitude'] = location.latitude\n",
    "    entry['longitude'] = location.longitude\n",
    "    locations.append(entry)\n",
    "locations = pd.DataFrame(locations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scales_to_plot = [int(np.floor(1.5*np.sqrt(s) + 0.5)) for s in scales]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import gmaps\n",
    "gmaps.configure(api_key=googlemaps_api_key)\n",
    "coordinates = (30, 0)\n",
    "fig = gmaps.figure(center=coordinates, zoom_level=2, layout={'width': '1000px', 'height': '600px'})\n",
    "\n",
    "\n",
    "startup_layer = gmaps.symbol_layer(\n",
    "    locations, fill_color='blue', stroke_color='blue', scale = scales_to_plot\n",
    ")\n",
    "fig.add_layer(startup_layer)\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#display a previously saved image\n",
    "#this can be useful if googlemaps has API key issues\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "plt.figure(figsize=(20,10))\n",
    "img=mpimg.imread('map.png')\n",
    "imgplot = plt.imshow(img)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3c Load failory data from json, and perform classification analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In part 3a we scraped www.failory.com to gather a dataset of interviews with startup founders. We had to do some processing to convert raw html to usable text (also in part 3a). Now the text data are stored in a .json file and we can more-or-less directly input these to the LSTM classifier which we trained in part 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data.json', 'r') as fp:\n",
    "    data = json.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#determine which tags failory has used\n",
    "failure_reason_tags = []\n",
    "area_tags = []\n",
    "\n",
    "for d in data:\n",
    "    \n",
    "    if('failure' in d['tags'].split()):\n",
    "        failure_reason_tags = failure_reason_tags + d['failory_tags'][2:3]\n",
    "        \n",
    "failure_reason_tags = np.array(failure_reason_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#failure_reason_tags\n",
    "\n",
    "unique_failure_reasons = np.unique(failure_reason_tags)\n",
    "counts = np.array([np.sum(failure_reason_tags==r) for r in unique_failure_reasons])\n",
    "order = np.argsort(counts)\n",
    "counts = counts[order]\n",
    "labels = unique_failure_reasons[order]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vals = np.arange(len(counts))\n",
    "plt.barh(vals,counts)\n",
    "plt.yticks(vals, [lab + \" \" for lab in labels])\n",
    "plt.title('Failure reasons, according to Failory\\n')\n",
    "plt.xlabel('Number of cases')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
